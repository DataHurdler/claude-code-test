{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Lab 11: Feature Engineering\n",
    "\n",
    "**BSAD 8310: Business Forecasting | University of Nebraska at Omaha**\n",
    "\n",
    "## Objectives\n",
    "\n",
    "1. Define a leakage-free `make_features_extended()` function (36 features: 12 lags, 12 rolling stats, 1 EWM, 11 month dummies)\n",
    "2. Visualize ACF/PACF lag structure and rolling feature time series\n",
    "3. Rank features by permutation importance (RF evaluated on validation set)\n",
    "4. Select features with LassoCV coefficient shrinkage and RFECV cross-validation curve\n",
    "5. Demonstrate leakage-free pipeline construction with `sklearn.pipeline.Pipeline`\n",
    "6. Quantify the RMSE impact of extended features vs. baseline features\n",
    "7. Update the full Lectures 01-11 model leaderboard\n",
    "\n",
    "## Packages Required\n",
    "```\n",
    "numpy, pandas, matplotlib, scikit-learn, statsmodels\n",
    "xgboost (optional), pandas_datareader (optional -- FRED data)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Section 1: Setup\n",
    "# =============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.linear_model import LassoCV, ElasticNet\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print('xgboost not installed -- XGBoost sections will be skipped.')\n",
    "    XGB_AVAILABLE = False\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# UNO color palette\n",
    "UNO = {\n",
    "    'blue':      '#005CA9',\n",
    "    'red':       '#E41C38',\n",
    "    'gray':      '#525252',\n",
    "    'green':     '#15803d',\n",
    "    'lightblue': '#cce0f5',\n",
    "    'lightgray': '#e5e5e5',\n",
    "    'lightred':  '#fce4e7',\n",
    "}\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'figure.dpi':        150,\n",
    "    'axes.spines.top':   False,\n",
    "    'axes.spines.right': False,\n",
    "    'font.size':         11,\n",
    "    'axes.titlesize':    13,\n",
    "})\n",
    "\n",
    "FIGURE_DIR = '../Figures'\n",
    "import os; os.makedirs(FIGURE_DIR, exist_ok=True)\n",
    "print('Setup complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Section 2: Load Data and Define Feature Functions\n",
    "# =============================================================================\n",
    "\n",
    "# --- Load RSXFS (US Retail Sales, FRED) ---\n",
    "try:\n",
    "    import pandas_datareader.data as web\n",
    "    raw = web.DataReader('RSXFS', 'fred',\n",
    "                         start='1992-01-01', end='2024-12-01')\n",
    "    df_raw = raw.rename(columns={'RSXFS': 'value'})\n",
    "    df_raw.index = pd.to_datetime(df_raw.index).to_period('M')\n",
    "    print(f'Loaded FRED RSXFS: {len(df_raw)} monthly observations')\n",
    "except Exception:\n",
    "    import statsmodels.api as sm\n",
    "    macro = sm.datasets.macrodata.load_pandas().data\n",
    "    macro.index = pd.period_range('1959Q1', periods=len(macro), freq='Q')\n",
    "    df_raw = pd.DataFrame({'value': macro['realgdp']})\n",
    "    print('Using statsmodels macrodata fallback (quarterly real GDP).')\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# Baseline feature function -- 29 features\n",
    "# (Identical to make_features() in Labs 07-10; labelled '~26f' in slides)\n",
    "# --------------------------------------------------------------------------\n",
    "def make_features_baseline(df, n_lags=12, roll_windows=(3, 6, 12)):\n",
    "    \"\"\"12 lags + mean/std per window (6 rolling) + 11 month dummies = 29 features.\"\"\"\n",
    "    y = df['value']\n",
    "    data = pd.DataFrame({'value': y})\n",
    "    for k in range(1, n_lags + 1):\n",
    "        data[f'lag_{k}'] = y.shift(k)\n",
    "    y_lag1 = y.shift(1)\n",
    "    for w in roll_windows:\n",
    "        data[f'roll_mean_{w}'] = y_lag1.rolling(w).mean()\n",
    "        data[f'roll_std_{w}']  = y_lag1.rolling(w).std()\n",
    "    if hasattr(y.index, 'to_timestamp'):\n",
    "        month_vals = y.index.to_timestamp().month\n",
    "    else:\n",
    "        month_vals = y.index.month\n",
    "    months = pd.Series(month_vals, index=y.index, dtype=int)\n",
    "    dums = pd.get_dummies(months, prefix='month', drop_first=True).astype(int)\n",
    "    data = pd.concat([data, dums], axis=1)\n",
    "    data.dropna(inplace=True)\n",
    "    return data.drop(columns='value'), data['value']\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# Extended feature function -- 36 features (as defined in Lecture 11 slides)\n",
    "# 12 lags + 4 rolling stats x 3 windows (12) + EWM (1) + 11 dummies = 36\n",
    "# --------------------------------------------------------------------------\n",
    "def make_features_extended(df, lags=range(1, 13), roll_windows=(3, 6, 12)):\n",
    "    \"\"\"Leakage-free extended features: all rolling ops use shift(1) first.\"\"\"\n",
    "    X = df[['value']].copy()\n",
    "    # Lag features\n",
    "    for k in lags:\n",
    "        X[f'lag_{k}'] = X['value'].shift(k)\n",
    "    # Rolling statistics (shift first to prevent look-ahead leakage)\n",
    "    for w in roll_windows:\n",
    "        r = X['value'].shift(1).rolling(w)\n",
    "        X[f'roll_mean_{w}'] = r.mean()\n",
    "        X[f'roll_std_{w}']  = r.std()\n",
    "        X[f'roll_min_{w}']  = r.min()\n",
    "        X[f'roll_max_{w}']  = r.max()\n",
    "    # Exponentially-weighted moving average (alpha=0.3; shift first)\n",
    "    X['ewm_alpha03'] = X['value'].shift(1).ewm(alpha=0.3, adjust=False).mean()\n",
    "    # Month dummies (drop_first=True: January is reference -> month_2 to month_12)\n",
    "    if hasattr(X.index, 'to_timestamp'):\n",
    "        month_vals = X.index.to_timestamp().month\n",
    "    else:\n",
    "        month_vals = X.index.month\n",
    "    months = pd.Series(month_vals, index=X.index, dtype=int)\n",
    "    dums = pd.get_dummies(months, prefix='month', drop_first=True).astype(int)\n",
    "    X = pd.concat([X, dums], axis=1)\n",
    "    X_out = X.drop(columns='value').dropna()\n",
    "    y_out  = df['value'].loc[X_out.index]\n",
    "    return X_out, y_out\n",
    "\n",
    "\n",
    "# Build both feature sets\n",
    "X_base, y_base = make_features_baseline(df_raw)\n",
    "X_ext,  y_ext  = make_features_extended(df_raw)\n",
    "\n",
    "# Align to common index (ensures fair comparison on identical observations)\n",
    "common_idx = X_base.index.intersection(X_ext.index)\n",
    "X_base, y_base = X_base.loc[common_idx], y_base.loc[common_idx]\n",
    "X_ext,  y_ext  = X_ext.loc[common_idx],  y_ext.loc[common_idx]\n",
    "\n",
    "print(f'Baseline features  : {X_base.shape[1]}  (labelled \"~26f\" in slides)')\n",
    "print(f'Extended features  : {X_ext.shape[1]}')\n",
    "print(f'Observations       : {len(y_ext)}')\n",
    "print(f'\\nExtended feature breakdown:')\n",
    "print(f'  Lags         ({len([c for c in X_ext.columns if c.startswith(\"lag_\")])}): lag_1 .. lag_12')\n",
    "print(f'  Roll mean    ({len([c for c in X_ext.columns if c.startswith(\"roll_mean\")])}): windows 3, 6, 12')\n",
    "print(f'  Roll std     ({len([c for c in X_ext.columns if c.startswith(\"roll_std\")])}): windows 3, 6, 12')\n",
    "print(f'  Roll min     ({len([c for c in X_ext.columns if c.startswith(\"roll_min\")])}): windows 3, 6, 12')\n",
    "print(f'  Roll max     ({len([c for c in X_ext.columns if c.startswith(\"roll_max\")])}): windows 3, 6, 12')\n",
    "print(f'  EWM          (1): alpha=0.3')\n",
    "print(f'  Month dummies({len([c for c in X_ext.columns if c.startswith(\"month_\")])}): month_2 .. month_12 (Jan=reference)')\n",
    "print(f'  TOTAL        : {X_ext.shape[1]}')\n",
    "\n",
    "# Three-way split\n",
    "n = len(X_ext)\n",
    "n_test  = int(0.15 * n)\n",
    "n_val   = int(0.15 * n)\n",
    "n_train = n - n_val - n_test\n",
    "\n",
    "def split_data(X, y):\n",
    "    Xtr  = X.iloc[:n_train]\n",
    "    ytr  = y.iloc[:n_train]\n",
    "    Xval = X.iloc[n_train:n_train + n_val]\n",
    "    yval = y.iloc[n_train:n_train + n_val]\n",
    "    Xte  = X.iloc[n_train + n_val:]\n",
    "    yte  = y.iloc[n_train + n_val:]\n",
    "    Xtv  = X.iloc[:n_train + n_val]\n",
    "    ytv  = y.iloc[:n_train + n_val]\n",
    "    return Xtr, ytr, Xval, yval, Xte, yte, Xtv, ytv\n",
    "\n",
    "Xtr, ytr, Xval, yval, Xte, yte, Xtv, ytv = split_data(X_ext, y_ext)\n",
    "Xb_tr, yb_tr, Xb_val, yb_val, Xb_te, yb_te, Xb_tv, yb_tv = split_data(X_base, y_base)\n",
    "\n",
    "print(f'\\nSplit: Train {n_train} | Val {n_val} | Test {n_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Section 3a: Feature Visualization -- ACF / PACF\n",
    "# =============================================================================\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "# Convert PeriodIndex to datetime for statsmodels\n",
    "if hasattr(y_ext.index, 'to_timestamp'):\n",
    "    y_plot = pd.Series(y_ext.values,\n",
    "                       index=y_ext.index.to_timestamp(), name='value')\n",
    "else:\n",
    "    y_plot = y_ext.copy()\n",
    "\n",
    "fig, (ax_acf, ax_pacf) = plt.subplots(2, 1, figsize=(10, 6))\n",
    "\n",
    "plot_acf(y_plot, lags=24, ax=ax_acf, color=UNO['blue'],\n",
    "         title='ACF of RSXFS (Monthly Retail Sales)',\n",
    "         zero=False, alpha=0.05)\n",
    "ax_acf.set_xlabel('')\n",
    "ax_acf.axhline(y=0, color=UNO['gray'], lw=0.8)\n",
    "\n",
    "plot_pacf(y_plot, lags=24, ax=ax_pacf, method='ywm', color=UNO['blue'],\n",
    "          title='PACF of RSXFS',\n",
    "          zero=False, alpha=0.05)\n",
    "ax_pacf.axhline(y=0, color=UNO['gray'], lw=0.8)\n",
    "\n",
    "# Highlight key lags\n",
    "for ax in [ax_acf, ax_pacf]:\n",
    "    ax.axvline(1,  color=UNO['red'],   ls='--', lw=1.2, alpha=0.5, label='Lag 1 (AR)')\n",
    "    ax.axvline(12, color=UNO['green'], ls='--', lw=1.2, alpha=0.5, label='Lag 12 (Seasonal)')\n",
    "    ax.legend(fontsize=9, loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{FIGURE_DIR}/lecture11_acf_pacf.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Saved lecture11_acf_pacf.png')\n",
    "print('\\nInterpretation:')\n",
    "print('  ACF: large spike at lag 12 -> strong seasonal pattern')\n",
    "print('  PACF: spike at lag 1 (AR component) and lag 12 (seasonal AR)')\n",
    "print('  -> Include lag_1, lag_2, lag_12 as minimum feature set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Section 3b: Feature Visualization -- Rolling Features Time Series\n",
    "# =============================================================================\n",
    "idx_str = X_ext.index.astype(str)\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(11, 8), sharex=True)\n",
    "\n",
    "# Panel 1: Actual series + lag_1\n",
    "ax = axes[0]\n",
    "ax.plot(idx_str, y_ext.values,           color='black',      lw=1.5, label='$y_t$ (actual)')\n",
    "ax.plot(idx_str, X_ext['lag_1'].values,  color=UNO['lightblue'], lw=1, ls='--', label='$y_{t-1}$ (lag 1)')\n",
    "ax.set_title('Lag Feature vs. Actual Series')\n",
    "ax.legend(fontsize=9); ax.set_ylabel('USD millions')\n",
    "\n",
    "# Panel 2: Rolling means at 3, 6, 12 months\n",
    "ax = axes[1]\n",
    "ax.plot(idx_str, X_ext['roll_mean_3'].values,  color=UNO['blue'],  lw=1.5, label='roll_mean_3')\n",
    "ax.plot(idx_str, X_ext['roll_mean_6'].values,  color=UNO['gray'],  lw=1.5, ls='--', label='roll_mean_6')\n",
    "ax.plot(idx_str, X_ext['roll_mean_12'].values, color=UNO['green'], lw=1.5, ls=':', label='roll_mean_12')\n",
    "ax.set_title('Rolling Means (3, 6, 12-month windows) -- all shifted by 1 to prevent leakage')\n",
    "ax.legend(fontsize=9); ax.set_ylabel('USD millions')\n",
    "\n",
    "# Panel 3: EWM vs. 3-month rolling mean\n",
    "ax = axes[2]\n",
    "ax.plot(idx_str, X_ext['ewm_alpha03'].values,  color=UNO['red'],  lw=1.5, label='EWM (alpha=0.3)')\n",
    "ax.plot(idx_str, X_ext['roll_mean_3'].values,  color=UNO['blue'], lw=1.5, ls='--', label='roll_mean_3')\n",
    "ax.set_title('EWM (alpha=0.3) vs. 3-Month Rolling Mean -- EWM reacts faster to recent changes')\n",
    "ax.legend(fontsize=9); ax.set_ylabel('USD millions')\n",
    "ax.set_xlabel('Period')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.xaxis.set_major_locator(mticker.MaxNLocator(8))\n",
    "plt.xticks(rotation=30)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{FIGURE_DIR}/lecture11_rolling_features.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Saved lecture11_rolling_features.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Section 4: Permutation Importance Ranking\n",
    "# Fit RF on training set; compute importance on validation set.\n",
    "# Permutation importance = mean RMSE increase when feature is shuffled.\n",
    "# =============================================================================\n",
    "\n",
    "rf_perm = RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "rf_perm.fit(Xtr, ytr)\n",
    "val_rmse = np.sqrt(mean_squared_error(yval, rf_perm.predict(Xval)))\n",
    "print(f'RF fitted. Validation RMSE (extended 36f): {val_rmse:,.1f}')\n",
    "\n",
    "# Permutation importance on validation set (unbiased -- val was not used for fitting)\n",
    "perm = permutation_importance(\n",
    "    rf_perm, Xval, yval,\n",
    "    n_repeats=10, random_state=42,\n",
    "    scoring='neg_root_mean_squared_error'\n",
    ")\n",
    "perm_df = pd.DataFrame({\n",
    "    'feature': X_ext.columns.tolist(),\n",
    "    'mean_increase': -perm.importances_mean,\n",
    "    'std':            perm.importances_std,\n",
    "}).sort_values('mean_increase', ascending=True)\n",
    "\n",
    "# Plot top 15 features\n",
    "n_top = 15\n",
    "perm_top = perm_df.tail(n_top)\n",
    "bar_colors = [\n",
    "    UNO['blue'] if v > 0 else UNO['lightgray']\n",
    "    for v in perm_top['mean_increase']\n",
    "]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 6))\n",
    "ax.barh(perm_top['feature'], perm_top['mean_increase'],\n",
    "        xerr=perm_top['std'],\n",
    "        color=bar_colors, edgecolor='white',\n",
    "        ecolor=UNO['gray'], capsize=3)\n",
    "ax.axvline(0, color='black', lw=0.8)\n",
    "ax.set_xlabel('Mean RMSE increase when feature shuffled (validation set)')\n",
    "ax.set_title('Permutation Feature Importance -- RF, Top 15 Features')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{FIGURE_DIR}/lecture11_permutation_importance.png',\n",
    "            dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Saved lecture11_permutation_importance.png')\n",
    "\n",
    "print('\\nTop 8 features by permutation importance:')\n",
    "print(perm_df.tail(8)[['feature', 'mean_increase', 'std']].to_string(index=False))\n",
    "print('\\nBottom 5 features (close to zero = less useful):')\n",
    "print(perm_df.head(5)[['feature', 'mean_increase', 'std']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Section 5a: Feature Selection -- LassoCV Coefficient Shrinkage\n",
    "# StandardScaler inside the fit (prevents leakage; required for LASSO)\n",
    "# =============================================================================\n",
    "tscv = TimeSeriesSplit(n_splits=5, gap=1)\n",
    "\n",
    "# Standardize features (LASSO penalizes coef magnitude, so scale matters)\n",
    "scaler_lasso = StandardScaler()\n",
    "Xtr_sc = scaler_lasso.fit_transform(Xtr)\n",
    "# Note: we fit scaler on training only -- validation/test are NOT used here\n",
    "\n",
    "lasso_cv = LassoCV(cv=tscv, max_iter=5000, random_state=42, n_jobs=-1)\n",
    "lasso_cv.fit(Xtr_sc, ytr)\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    'feature': X_ext.columns.tolist(),\n",
    "    'coef':    lasso_cv.coef_\n",
    "})\n",
    "selected = coef_df[coef_df['coef'] != 0].sort_values('coef', key=abs, ascending=False)\n",
    "zero_coef = coef_df[coef_df['coef'] == 0]\n",
    "\n",
    "print(f'LassoCV selected lambda = {lasso_cv.alpha_:.4f}')\n",
    "print(f'Features retained : {len(selected)} / {X_ext.shape[1]}')\n",
    "print(f'Features zeroed   : {len(zero_coef)} / {X_ext.shape[1]}')\n",
    "print(f'\\nTop 10 retained features (by |coef|):')\n",
    "print(selected.head(10).to_string(index=False))\n",
    "if len(zero_coef) > 0:\n",
    "    print(f'\\nZeroed features: {zero_coef[\"feature\"].tolist()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Section 5b: Feature Selection -- RFECV (RF-based, CV curve)\n",
    "# Uses TimeSeriesSplit to respect temporal ordering.\n",
    "# RFECV: recursively eliminates least-important features, picks n by CV RMSE.\n",
    "# Note: may take 1-3 minutes with n_estimators=50.\n",
    "# =============================================================================\n",
    "rf_for_rfe = RandomForestRegressor(\n",
    "    n_estimators=50, max_depth=5, random_state=42, n_jobs=-1\n",
    ")\n",
    "\n",
    "rfecv = RFECV(\n",
    "    estimator=rf_for_rfe,\n",
    "    step=1,\n",
    "    cv=tscv,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    min_features_to_select=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "print('Fitting RFECV (this may take 1-2 minutes)...')\n",
    "rfecv.fit(Xtv, ytv)\n",
    "\n",
    "n_opt = rfecv.n_features_\n",
    "selected_features = X_ext.columns[rfecv.support_].tolist()\n",
    "print(f'Optimal number of features: {n_opt} / {X_ext.shape[1]}')\n",
    "print(f'Selected features: {selected_features}')\n",
    "\n",
    "# Plot CV RMSE vs. number of features selected\n",
    "try:\n",
    "    cv_scores = -rfecv.cv_results_['mean_test_score']  # sklearn >= 1.0\n",
    "except AttributeError:\n",
    "    cv_scores = -rfecv.grid_scores_  # sklearn < 1.0\n",
    "\n",
    "n_feat_range = np.arange(rfecv.min_features_to_select,\n",
    "                          rfecv.min_features_to_select + len(cv_scores))\n",
    "\n",
    "opt_idx = n_opt - rfecv.min_features_to_select\n",
    "opt_rmse = cv_scores[opt_idx]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 4))\n",
    "ax.plot(n_feat_range, cv_scores, 'o-', color=UNO['blue'], lw=2, ms=4)\n",
    "ax.scatter([n_opt], [opt_rmse], color=UNO['red'], s=80, zorder=5)\n",
    "ax.axvline(n_opt, color=UNO['red'], ls='--', lw=1.5,\n",
    "           label=f'Optimal: {n_opt} features (CV RMSE = {opt_rmse:,.0f})')\n",
    "ax.set_xlabel('Number of features selected')\n",
    "ax.set_ylabel('CV RMSE (TimeSeriesSplit, gap=1)')\n",
    "ax.set_title('RFECV: Cross-Validated RMSE vs. Number of Features (RF estimator)')\n",
    "ax.legend(fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{FIGURE_DIR}/lecture11_rfecv_curve.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Saved lecture11_rfecv_curve.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Section 6: Pipeline Construction + CV Leakage Demonstration\n",
    "#\n",
    "# Key point: StandardScaler MUST be inside the Pipeline so it is fit only\n",
    "# on the training fold of each CV split, not on validation data.\n",
    "#\n",
    "# Demonstration with ElasticNet (where scaling has a visible effect on CV RMSE).\n",
    "# =============================================================================\n",
    "tscv_pipe = TimeSeriesSplit(n_splits=5, gap=1)\n",
    "\n",
    "# --- CORRECT: StandardScaler inside Pipeline ---\n",
    "pipe_correct = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model',  ElasticNet(alpha=0.1, l1_ratio=0.5, max_iter=5000, random_state=42))\n",
    "])\n",
    "scores_correct = cross_val_score(\n",
    "    pipe_correct, Xtv.values, ytv.values,\n",
    "    cv=tscv_pipe, scoring='neg_root_mean_squared_error'\n",
    ")\n",
    "rmse_correct = -scores_correct.mean()\n",
    "\n",
    "# --- LEAKY: Scaler fit on full train+val data BEFORE CV loop ---\n",
    "scaler_leaky = StandardScaler()\n",
    "Xtv_leaky = scaler_leaky.fit_transform(Xtv)  # Leaks! Sees statistics from all folds\n",
    "\n",
    "model_leaky = ElasticNet(alpha=0.1, l1_ratio=0.5, max_iter=5000, random_state=42)\n",
    "scores_leaky = cross_val_score(\n",
    "    model_leaky, Xtv_leaky, ytv.values,\n",
    "    cv=tscv_pipe, scoring='neg_root_mean_squared_error'\n",
    ")\n",
    "rmse_leaky = -scores_leaky.mean()\n",
    "\n",
    "print('Pipeline Leakage Demonstration (ElasticNet + StandardScaler)')\n",
    "print('=' * 60)\n",
    "print(f'Correct (scaler inside Pipeline) : CV RMSE = {rmse_correct:,.1f}')\n",
    "print(f'Leaky   (scaler fit before CV)   : CV RMSE = {rmse_leaky:,.1f}')\n",
    "if rmse_leaky < rmse_correct:\n",
    "    print(f'\\n*** Leaky approach reports {rmse_correct - rmse_leaky:,.1f} lower CV RMSE -- artificially optimistic!')\n",
    "    print('    The reported RMSE would NOT generalize to new data.')\n",
    "else:\n",
    "    print('\\nNote: With ElasticNet the leakage effect is mild on this dataset.')\n",
    "    print('It is more dramatic with target encoding or imputation inside the CV loop.')\n",
    "\n",
    "# --- Production pipeline: RF + StandardScaler (RF is scale-invariant but Pipeline is good practice) ---\n",
    "pipe_rf = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model',  RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1))\n",
    "])\n",
    "scores_pipe_rf = cross_val_score(\n",
    "    pipe_rf, Xtv.values, ytv.values,\n",
    "    cv=tscv_pipe, scoring='neg_root_mean_squared_error'\n",
    ")\n",
    "print(f'\\nProduction Pipeline (RF + StandardScaler) : CV RMSE = {-scores_pipe_rf.mean():,.1f}')\n",
    "print('(RF is scale-invariant so scaler has no effect on RF RMSE, but using Pipeline is good practice.)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Section 7: Model Comparison -- Baseline (29f) vs. Extended (36f)\n",
    "# Fit RF and XGBoost on train+val; evaluate on held-out test set.\n",
    "# =============================================================================\n",
    "\n",
    "def rmse(a, p):\n",
    "    a, p = np.asarray(a), np.asarray(p)[:len(a)]\n",
    "    return np.sqrt(mean_squared_error(a, p))\n",
    "\n",
    "def mae(a, p):\n",
    "    a, p = np.asarray(a), np.asarray(p)[:len(a)]\n",
    "    return mean_absolute_error(a, p)\n",
    "\n",
    "results = []\n",
    "pred_rf_ext  = None\n",
    "pred_xgb_ext = None\n",
    "\n",
    "# --- Random Forest: baseline (29f) ---\n",
    "rf_base = RandomForestRegressor(n_estimators=500, random_state=42, n_jobs=-1)\n",
    "rf_base.fit(Xb_tv, yb_tv)\n",
    "pred_rf_base = rf_base.predict(Xb_te)\n",
    "results.append(('RF (29f baseline)',\n",
    "                rmse(yb_te, pred_rf_base), mae(yb_te, pred_rf_base)))\n",
    "\n",
    "# --- Random Forest: extended (36f) ---\n",
    "rf_ext = RandomForestRegressor(n_estimators=500, random_state=42, n_jobs=-1)\n",
    "rf_ext.fit(Xtv, ytv)\n",
    "pred_rf_ext = rf_ext.predict(Xte)\n",
    "results.append(('RF (36f extended)',\n",
    "                rmse(yte, pred_rf_ext), mae(yte, pred_rf_ext)))\n",
    "\n",
    "# --- XGBoost: baseline and extended ---\n",
    "if XGB_AVAILABLE:\n",
    "    xgb_params = {\n",
    "        'learning_rate': 0.05, 'max_depth': 4,\n",
    "        'subsample': 0.8, 'colsample_bytree': 0.8,\n",
    "        'reg_lambda': 1.0, 'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'rmse', 'seed': 42,\n",
    "    }\n",
    "    # Baseline\n",
    "    xgb_b = xgb.train(xgb_params,\n",
    "                       xgb.DMatrix(Xb_tr, label=yb_tr), 2000,\n",
    "                       evals=[(xgb.DMatrix(Xb_val, label=yb_val), 'val')],\n",
    "                       early_stopping_rounds=50, verbose_eval=False)\n",
    "    xgb_b_final = xgb.train(xgb_params, xgb.DMatrix(Xb_tv, label=yb_tv),\n",
    "                              xgb_b.best_iteration)\n",
    "    pred_xgb_base = xgb_b_final.predict(xgb.DMatrix(Xb_te))\n",
    "    results.append(('XGB (29f baseline)',\n",
    "                    rmse(yb_te, pred_xgb_base), mae(yb_te, pred_xgb_base)))\n",
    "    # Extended\n",
    "    xgb_e = xgb.train(xgb_params,\n",
    "                       xgb.DMatrix(Xtr, label=ytr), 2000,\n",
    "                       evals=[(xgb.DMatrix(Xval, label=yval), 'val')],\n",
    "                       early_stopping_rounds=50, verbose_eval=False)\n",
    "    xgb_e_final = xgb.train(xgb_params, xgb.DMatrix(Xtv, label=ytv),\n",
    "                              xgb_e.best_iteration)\n",
    "    pred_xgb_ext = xgb_e_final.predict(xgb.DMatrix(Xte))\n",
    "    results.append(('XGB (36f extended)',\n",
    "                    rmse(yte, pred_xgb_ext), mae(yte, pred_xgb_ext)))\n",
    "\n",
    "res_df = pd.DataFrame(results, columns=['Model', 'RMSE', 'MAE'])\n",
    "res_df['RMSE'] = res_df['RMSE'].round(1)\n",
    "res_df['MAE']  = res_df['MAE'].round(1)\n",
    "print('Feature engineering impact on test-set performance:')\n",
    "print(res_df.to_string(index=False))\n",
    "\n",
    "# --- Bar chart ---\n",
    "n_models = len(res_df)\n",
    "bar_colors = ([UNO['lightblue'], UNO['blue']] +\n",
    "              ([UNO['lightred'], UNO['red']] if XGB_AVAILABLE else []))[:n_models]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 4))\n",
    "bars = ax.bar(res_df['Model'], res_df['RMSE'],\n",
    "              color=bar_colors, edgecolor='white', width=0.6)\n",
    "for bar, val in zip(bars, res_df['RMSE']):\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 15,\n",
    "            f'{val:,.0f}', ha='center', va='bottom', fontsize=10)\n",
    "ax.set_ylabel('Test RMSE (USD millions)')\n",
    "ax.set_title('Feature Engineering Impact: Baseline (~29f) vs. Extended (36f)')\n",
    "ax.set_ylim(0, res_df['RMSE'].max() * 1.15)\n",
    "plt.xticks(rotation=15)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{FIGURE_DIR}/lecture11_feature_impact.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Saved lecture11_feature_impact.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Section 8: Full Leaderboard -- Lectures 01-11\n",
    "# Illustrative values consistent with slides (actual values vary by sample period).\n",
    "# ML models below L08 use the extended 36-feature set from this lab.\n",
    "# =============================================================================\n",
    "\n",
    "leaderboard = pd.DataFrame([\n",
    "    ('L01', 'Seasonal Naive',                4210, 3120, 'classical'),\n",
    "    ('L01', 'Seasonal Mean',                 3890, 2840, 'classical'),\n",
    "    ('L03', 'ETS (Holt-Winters)',             3210, 2350, 'classical'),\n",
    "    ('L04', 'SARIMA(1,1,1)(1,1,1)_12',       2840, 2100, 'classical'),\n",
    "    ('L05', 'ARIMAX (retail index)',          2720, 2010, 'classical'),\n",
    "    ('L08', 'Elastic Net (~29f)',             2540, 1890, 'ml_base'),\n",
    "    ('L09', 'Random Forest (~29f)',           2380, 1760, 'ml_base'),\n",
    "    ('L09', 'XGBoost (~29f)',                 2250, 1650, 'ml_base'),\n",
    "    ('L10', 'LSTM (med, 5 seeds, ~29f)',      2180, 1600, 'ml_base'),\n",
    "    ('L11', 'Elastic Net (36f)',              2410, 1820, 'ml_ext'),\n",
    "    ('L11', 'Random Forest (36f)',            2210, 1640, 'ml_ext'),\n",
    "    ('L11', 'XGBoost (36f)',                  2050, 1510, 'ml_ext'),\n",
    "    ('L11', 'LSTM (med, 5 seeds, 36f)',       1920, 1430, 'ml_ext'),\n",
    "], columns=['Lecture', 'Model', 'RMSE', 'MAE', 'group'])\n",
    "\n",
    "leaderboard_sorted = leaderboard.sort_values('RMSE', ascending=True).reset_index(drop=True)\n",
    "\n",
    "group_colors = {\n",
    "    'classical': UNO['gray'],\n",
    "    'ml_base':   UNO['lightblue'],\n",
    "    'ml_ext':    UNO['blue'],\n",
    "}\n",
    "bar_colors = [group_colors[g] for g in leaderboard_sorted['group']]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(11, 5))\n",
    "bars = ax.barh(leaderboard_sorted['Model'], leaderboard_sorted['RMSE'],\n",
    "               color=bar_colors, edgecolor='white')\n",
    "for bar, val in zip(bars, leaderboard_sorted['RMSE']):\n",
    "    ax.text(val + 15, bar.get_y() + bar.get_height() / 2,\n",
    "            f'{val:,}', va='center', fontsize=9)\n",
    "\n",
    "ax.set_xlabel('Test RMSE (USD millions) -- illustrative values from slides')\n",
    "ax.set_title('Model Leaderboard: Lectures 01-11')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor=UNO['gray'],      label='Classical methods (L01-L05)'),\n",
    "    Patch(facecolor=UNO['lightblue'], label='ML with baseline features (~29f, L08-L10)'),\n",
    "    Patch(facecolor=UNO['blue'],      label='ML with extended features (36f, L11)'),\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='lower right', fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{FIGURE_DIR}/lecture11_leaderboard.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Saved lecture11_leaderboard.png')\n",
    "print('\\nFull leaderboard (sorted by RMSE):')\n",
    "print(leaderboard_sorted[['Lecture', 'Model', 'RMSE', 'MAE']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Section 9: Forecast Comparison Plot\n",
    "# Actuals + RF(36f) + XGBoost(36f) over the test period.\n",
    "# LSTM(36f) shown as illustrative series (RMSE ~1,920 from Lecture 10 replication).\n",
    "# =============================================================================\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(11, 4))\n",
    "\n",
    "# History context: last 24 observations of train+val\n",
    "context_y = ytv.iloc[-24:]\n",
    "context_idx = context_y.index.astype(str)\n",
    "ax.plot(context_idx, context_y.values,\n",
    "        color=UNO['lightgray'], lw=1.5, label='History')\n",
    "\n",
    "# Test actuals\n",
    "test_idx = yte.index.astype(str)\n",
    "ax.plot(test_idx, yte.values,\n",
    "        color='black', lw=2, label='Actual', zorder=5)\n",
    "\n",
    "# RF (36f)\n",
    "if pred_rf_ext is not None:\n",
    "    r_rf = rmse(yte, pred_rf_ext)\n",
    "    ax.plot(test_idx, pred_rf_ext,\n",
    "            color=UNO['blue'], lw=2, ls='-.', label=f'RF 36f (RMSE={r_rf:,.0f})')\n",
    "\n",
    "# XGBoost (36f)\n",
    "if XGB_AVAILABLE and pred_xgb_ext is not None:\n",
    "    r_xgb = rmse(yte, pred_xgb_ext)\n",
    "    ax.plot(test_idx, pred_xgb_ext,\n",
    "            color=UNO['red'], lw=2, label=f'XGB 36f (RMSE={r_xgb:,.0f})')\n",
    "\n",
    "# LSTM (36f): illustrative -- shown as smoothed XGBoost predictions\n",
    "# Actual LSTM values require TensorFlow; see Lecture 10 notebook for full implementation.\n",
    "# The smoothed series below approximates LSTM's tendency to dampen sharp fluctuations.\n",
    "if XGB_AVAILABLE and pred_xgb_ext is not None:\n",
    "    lstm_approx = pd.Series(pred_xgb_ext).ewm(span=4).mean().values\n",
    "    ax.plot(test_idx, lstm_approx,\n",
    "            color=UNO['green'], lw=2, ls='--',\n",
    "            label='LSTM 36f (RMSE ~1,920, illustrative -- see L10 for full implementation)')\n",
    "\n",
    "ax.set_title('Forecast Comparison: Extended Feature Set (Test Set)')\n",
    "ax.set_xlabel('Period')\n",
    "ax.set_ylabel('Retail Sales (USD millions)')\n",
    "ax.legend(loc='upper left', fontsize=9)\n",
    "ax.xaxis.set_major_locator(mticker.MaxNLocator(8))\n",
    "plt.xticks(rotation=30)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{FIGURE_DIR}/lecture11_forecast_comparison.png',\n",
    "            dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Saved lecture11_forecast_comparison.png')\n",
    "\n",
    "print('\\n=== Lab 11 Complete ===')\n",
    "print(f'Figures saved to: {FIGURE_DIR}/')\n",
    "print('lecture11_acf_pacf.png')\n",
    "print('lecture11_rolling_features.png')\n",
    "print('lecture11_permutation_importance.png')\n",
    "print('lecture11_rfecv_curve.png')\n",
    "print('lecture11_feature_impact.png')\n",
    "print('lecture11_leaderboard.png')\n",
    "print('lecture11_forecast_comparison.png')"
   ]
  }
 ]
}
