{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-md-header",
   "metadata": {},
   "source": [
    "# Lab 07: Machine Learning Introduction\n",
    "**BSAD 8310: Business Forecasting — University of Nebraska at Omaha**\n",
    "\n",
    "## Objectives\n",
    "1. Build a feature matrix from a raw time series (lags, rolling stats, calendar features)\n",
    "2. Perform a proper three-way train/validation/test split with chronological ordering\n",
    "3. Implement walk-forward baseline (Seasonal Naive, SARIMA)\n",
    "4. Tune Ridge and LASSO with `TimeSeriesSplit` cross-validation\n",
    "5. Visualise the bias-variance tradeoff using a synthetic experiment\n",
    "6. Compare all models on horizon RMSE profiles\n",
    "7. Plot LASSO coefficient paths to illustrate variable selection\n",
    "\n",
    "## Dataset\n",
    "- **RSXFS**: Advance Retail Sales — Retail and Food Services (monthly, SA, millions USD)\n",
    "- Source: FRED (Federal Reserve Bank of St. Louis)\n",
    "- Fallback: statsmodels macrodata (quarterly GDP)\n",
    "\n",
    "## Key packages\n",
    "`numpy`, `pandas`, `matplotlib`, `statsmodels`, `scikit-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 1. Setup ──────────────────────────────────────────────────────────────────\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "\n",
    "# sklearn imports\n",
    "from sklearn.linear_model import Ridge, Lasso, lasso_path\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# UNO color palette\n",
    "UNO_BLUE   = '#005CA9'\n",
    "UNO_RED    = '#E41C38'\n",
    "UNO_GRAY   = '#525252'\n",
    "UNO_GREEN  = '#15803d'\n",
    "UNO_ORANGE = '#d97706'\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'figure.dpi': 150,\n",
    "    'axes.spines.top': False,\n",
    "    'axes.spines.right': False,\n",
    "    'axes.titlesize': 11,\n",
    "    'axes.labelsize': 10,\n",
    "    'legend.fontsize': 9,\n",
    "})\n",
    "\n",
    "FIGURES = Path('../Figures')\n",
    "FIGURES.mkdir(exist_ok=True)\n",
    "\n",
    "print('Setup complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 2. Load Data ──────────────────────────────────────────────────────────────\n",
    "try:\n",
    "    import pandas_datareader.data as web\n",
    "    start, end = '2000-01-01', '2023-12-31'\n",
    "    rsxfs  = web.DataReader('RSXFS', 'fred', start, end)\n",
    "    series = rsxfs['RSXFS'].dropna()\n",
    "    series.index = pd.PeriodIndex(series.index, freq='M')\n",
    "    m    = 12\n",
    "    FREQ = 'M'\n",
    "    print(f'FRED RSXFS: {len(series)} months, {series.index[0]} to {series.index[-1]}')\n",
    "except Exception as e:\n",
    "    print(f'FRED unavailable ({e}); using statsmodels macrodata fallback.')\n",
    "    import statsmodels.api as sm\n",
    "    macro  = sm.datasets.macrodata.load_pandas().data\n",
    "    series = macro['realgdp'].copy()\n",
    "    series.index = pd.period_range('1959Q1', periods=len(series), freq='Q')\n",
    "    m    = 4\n",
    "    FREQ = 'Q'\n",
    "    print(f'Fallback: {len(series)} quarters, {series.index[0]} to {series.index[-1]}')\n",
    "\n",
    "print(series.describe().round(1))\n",
    "\n",
    "# Quick visualisation\n",
    "fig, ax = plt.subplots(figsize=(11, 3))\n",
    "ax.plot(series.index.to_timestamp(), series.values, color=UNO_BLUE, lw=1.2)\n",
    "ax.set_title('Raw Series', fontweight='bold')\n",
    "ax.set_ylabel('Value')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3-features",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 3. Feature Engineering ────────────────────────────────────────────────────\n",
    "def make_features(y, lags=None, roll_windows=None, add_calendar=True):\n",
    "    \"\"\"\n",
    "    Build a feature matrix from a time series.\n",
    "\n",
    "    All features use .shift(1) or later to prevent leakage:\n",
    "    when predicting y_t, we can only use y_{t-1} and earlier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y            : pd.Series with PeriodIndex\n",
    "    lags         : list of int lag orders\n",
    "    roll_windows : list of int rolling window sizes\n",
    "    add_calendar : bool, add month-of-year feature\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X : pd.DataFrame of features (NaN rows dropped)\n",
    "    y_aligned : pd.Series aligned with X (same index)\n",
    "    \"\"\"\n",
    "    if lags is None:\n",
    "        lags = [1, 2, 3, 6, 12]\n",
    "    if roll_windows is None:\n",
    "        roll_windows = [3, 6, 12]\n",
    "\n",
    "    df = pd.DataFrame(index=y.index)\n",
    "\n",
    "    # Lag features: x_t^(k) = y_{t-k}\n",
    "    for k in lags:\n",
    "        df[f'lag_{k}'] = y.shift(k)\n",
    "\n",
    "    # Rolling mean and std (applied after shift(1) to avoid leakage)\n",
    "    y_shifted = y.shift(1)\n",
    "    for w in roll_windows:\n",
    "        df[f'roll_mean_{w}'] = y_shifted.rolling(w).mean()\n",
    "        df[f'roll_std_{w}']  = y_shifted.rolling(w).std()\n",
    "\n",
    "    # Calendar feature (always known in advance — no leakage)\n",
    "    if add_calendar and hasattr(y.index, 'month'):\n",
    "        df['month_sin'] = np.sin(2 * np.pi * y.index.month / 12)\n",
    "        df['month_cos'] = np.cos(2 * np.pi * y.index.month / 12)\n",
    "\n",
    "    # Trend counter\n",
    "    df['trend'] = np.arange(len(y))\n",
    "\n",
    "    # Drop rows with NaN (from lags and rolling windows)\n",
    "    valid_idx = df.dropna().index\n",
    "    return df.loc[valid_idx], y.loc[valid_idx]\n",
    "\n",
    "\n",
    "X_full, y_full = make_features(\n",
    "    series,\n",
    "    lags=[1, 2, 3, 6, 12],\n",
    "    roll_windows=[3, 6, 12]\n",
    ")\n",
    "\n",
    "print(f'Feature matrix shape: {X_full.shape}')\n",
    "print(f'Features: {list(X_full.columns)}')\n",
    "print(X_full.head(3).round(1))\n",
    "\n",
    "# Visualise two key features vs. the target\n",
    "fig, axes = plt.subplots(1, 2, figsize=(11, 3))\n",
    "for ax, feat, color in zip(axes, ['lag_1', 'roll_mean_12'],\n",
    "                            [UNO_BLUE, UNO_GREEN]):\n",
    "    ax.plot(X_full.index.to_timestamp(), X_full[feat],\n",
    "            color=color, lw=1.0, label=feat)\n",
    "    ax.plot(y_full.index.to_timestamp(), y_full.values,\n",
    "            color=UNO_GRAY, lw=0.8, alpha=0.5, label='y_t')\n",
    "    ax.set_title(feat, fontweight='bold')\n",
    "    ax.legend(fontsize=8)\n",
    "plt.suptitle('Selected Features vs. Target', fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4-split",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 4. Three-Way Train / Validation / Test Split ──────────────────────────────\n",
    "def time_split(X, y, val_frac=0.15, test_frac=0.15):\n",
    "    \"\"\"\n",
    "    Chronological three-way split.\n",
    "    Returns: (X_tr, y_tr), (X_va, y_va), (X_te, y_te)\n",
    "    \"\"\"\n",
    "    n = len(X)\n",
    "    n_test = int(np.floor(n * test_frac))\n",
    "    n_val  = int(np.floor(n * val_frac))\n",
    "    n_tr   = n - n_val - n_test\n",
    "    return (\n",
    "        (X.iloc[:n_tr],             y.iloc[:n_tr]),\n",
    "        (X.iloc[n_tr:n_tr+n_val],   y.iloc[n_tr:n_tr+n_val]),\n",
    "        (X.iloc[n_tr+n_val:],        y.iloc[n_tr+n_val:]),\n",
    "    )\n",
    "\n",
    "\n",
    "(X_tr, y_tr), (X_va, y_va), (X_te, y_te) = time_split(\n",
    "    X_full, y_full, val_frac=0.15, test_frac=0.15)\n",
    "\n",
    "print(f'Train:      {len(X_tr):4d} obs  {y_tr.index[0]} – {y_tr.index[-1]}')\n",
    "print(f'Validation: {len(X_va):4d} obs  {y_va.index[0]} – {y_va.index[-1]}')\n",
    "print(f'Test:       {len(X_te):4d} obs  {y_te.index[0]} – {y_te.index[-1]}')\n",
    "\n",
    "# Fit scaler on train only; transform val and test\n",
    "scaler = StandardScaler()\n",
    "X_tr_s = scaler.fit_transform(X_tr)\n",
    "X_va_s = scaler.transform(X_va)\n",
    "X_te_s = scaler.transform(X_te)\n",
    "\n",
    "# Visualise the split as a horizontal bar\n",
    "fig, ax = plt.subplots(figsize=(10, 1.4))\n",
    "sizes  = [len(X_tr), len(X_va), len(X_te)]\n",
    "labels = ['Train', 'Validation', 'Test']\n",
    "colors = [UNO_BLUE, UNO_GREEN, UNO_RED]\n",
    "left = 0\n",
    "for size, label, color in zip(sizes, labels, colors):\n",
    "    ax.barh(0, size, left=left, color=color, height=0.5)\n",
    "    ax.text(left + size/2, 0, f'{label}\\n({size})',\n",
    "            ha='center', va='center', color='white', fontsize=9)\n",
    "    left += size\n",
    "ax.set_xlim(0, len(X_full))\n",
    "ax.axis('off')\n",
    "ax.set_title('Chronological Train / Validation / Test Split',\n",
    "             fontweight='bold', fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5-baseline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 5. Baseline: Walk-Forward Naive and SARIMA ────────────────────────────────\n",
    "# Re-use walk_forward_eval from L06 (inlined here for self-containment)\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "def walk_forward_eval(series_in, model_fn, T0, H,\n",
    "                      window='expanding', window_size=None):\n",
    "    \"\"\"Walk-forward evaluation returning a DataFrame of errors.\"\"\"\n",
    "    records = []\n",
    "    n = len(series_in)\n",
    "    for t in range(T0, n - H):\n",
    "        if window == 'expanding':\n",
    "            train = series_in.iloc[:t]\n",
    "        else:\n",
    "            start = max(0, t - (window_size or T0))\n",
    "            train = series_in.iloc[start:t]\n",
    "        try:\n",
    "            fc = model_fn(train)\n",
    "        except Exception:\n",
    "            fc = np.full(H, np.nan)\n",
    "        for h in range(1, H + 1):\n",
    "            actual = float(series_in.iloc[t + h - 1])\n",
    "            fcast  = float(fc[h - 1]) if h <= len(fc) else np.nan\n",
    "            records.append({'origin': t, 'horizon': h,\n",
    "                            'actual': actual, 'forecast': fcast,\n",
    "                            'error':  actual - fcast})\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "\n",
    "H  = 12\n",
    "T0 = len(series) - 36 - H   # 36 walk-forward origins\n",
    "\n",
    "def seasonal_naive_fn(train):\n",
    "    last_season = train.values[-m:]\n",
    "    return np.tile(last_season, (H // m) + 1)[:H]\n",
    "\n",
    "def sarima_fn(train):\n",
    "    mod = SARIMAX(train.values,\n",
    "                  order=(1, 1, 1),\n",
    "                  seasonal_order=(0, 1, 1, m),\n",
    "                  enforce_stationarity=False,\n",
    "                  enforce_invertibility=False)\n",
    "    return mod.fit(disp=False).forecast(H)\n",
    "\n",
    "print(f'Walk-forward: T0={T0}, {len(series)-T0-H} origins, H={H}')\n",
    "print('Seasonal Naive...', end=' ')\n",
    "wf_naive  = walk_forward_eval(series, seasonal_naive_fn, T0, H)\n",
    "print('done.  SARIMA...', end=' ')\n",
    "wf_sarima = walk_forward_eval(series, sarima_fn, T0, H)\n",
    "print('done.')\n",
    "\n",
    "def horizon_rmse(df):\n",
    "    return df.dropna().groupby('horizon').apply(\n",
    "        lambda g: np.sqrt(np.mean((g['actual'] - g['forecast'])**2))\n",
    "    )\n",
    "\n",
    "naive_profile  = horizon_rmse(wf_naive)\n",
    "sarima_profile = horizon_rmse(wf_sarima)\n",
    "\n",
    "print('\\nBaseline RMSE at h=1, 3, 12:')\n",
    "for h in [1, 3, 12]:\n",
    "    print(f'  h={h:2d}:  Naive={naive_profile[h]:,.0f}  \"\\\n",
    "          \"SARIMA={sarima_profile[h]:,.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6-ridge-lasso",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 6. Ridge and LASSO with TimeSeriesSplit CV ────────────────────────────────\n",
    "# Use the train+validation window for CV; reserve test for final evaluation\n",
    "X_trainval   = np.vstack([X_tr_s, X_va_s])\n",
    "y_trainval   = np.concatenate([y_tr.values, y_va.values])\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5, gap=0)\n",
    "\n",
    "def cv_rmse(model_cls, alpha, X, y):\n",
    "    \"\"\"Return mean cross-validated RMSE for a penalised model.\"\"\"\n",
    "    rmse_scores = []\n",
    "    for tr_idx, va_idx in tscv.split(X):\n",
    "        m_fit = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('model',  model_cls(alpha=alpha))\n",
    "        ])\n",
    "        m_fit.fit(X[tr_idx], y[tr_idx])\n",
    "        pred = m_fit.predict(X[va_idx])\n",
    "        rmse_scores.append(np.sqrt(mean_squared_error(y[va_idx], pred)))\n",
    "    return np.mean(rmse_scores)\n",
    "\n",
    "\n",
    "ridge_alphas = np.logspace(-2, 4, 30)\n",
    "lasso_alphas = np.logspace(-1, 5, 30)\n",
    "\n",
    "print('Tuning Ridge...', end=' ')\n",
    "ridge_cv = [cv_rmse(Ridge, a, X_trainval, y_trainval) for a in ridge_alphas]\n",
    "alpha_ridge_star = ridge_alphas[np.argmin(ridge_cv)]\n",
    "print(f'alpha* = {alpha_ridge_star:.2f}')\n",
    "\n",
    "print('Tuning LASSO...', end=' ')\n",
    "lasso_cv = [cv_rmse(Lasso, a, X_trainval, y_trainval) for a in lasso_alphas]\n",
    "alpha_lasso_star = lasso_alphas[np.argmin(lasso_cv)]\n",
    "print(f'alpha* = {alpha_lasso_star:.2f}')\n",
    "\n",
    "# Plot CV curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(11, 3.5))\n",
    "for ax, alphas, cv_vals, a_star, name, color in zip(\n",
    "    axes,\n",
    "    [ridge_alphas, lasso_alphas],\n",
    "    [ridge_cv, lasso_cv],\n",
    "    [alpha_ridge_star, alpha_lasso_star],\n",
    "    ['Ridge', 'LASSO'],\n",
    "    [UNO_BLUE, UNO_RED]\n",
    "):\n",
    "    ax.semilogx(alphas, cv_vals, color=color, lw=1.8)\n",
    "    ax.axvline(a_star, color='black', ls='--', lw=1.0,\n",
    "               label=f'$\\\\alpha^* = {a_star:.2f}$')\n",
    "    ax.set_xlabel('Regularisation strength $\\\\alpha$')\n",
    "    ax.set_ylabel('Mean CV RMSE')\n",
    "    ax.set_title(f'{name} — CV Tuning', fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.yaxis.set_major_formatter(\n",
    "        mticker.FuncFormatter(lambda x, _: f'{x:,.0f}'))\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES / 'lecture07_cv_tuning.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Refit on full train+val, evaluate on test\n",
    "scaler_tv = StandardScaler().fit(X_trainval)\n",
    "X_tv_s    = scaler_tv.transform(X_trainval)\n",
    "X_te_s2   = scaler_tv.transform(X_te.values)\n",
    "\n",
    "ridge_final = Ridge(alpha=alpha_ridge_star).fit(X_tv_s, y_trainval)\n",
    "lasso_final = Lasso(alpha=alpha_lasso_star, max_iter=10000).fit(X_tv_s, y_trainval)\n",
    "\n",
    "ridge_pred  = ridge_final.predict(X_te_s2)\n",
    "lasso_pred  = lasso_final.predict(X_te_s2)\n",
    "\n",
    "def rmse(a, f):\n",
    "    return np.sqrt(np.mean((np.asarray(a) - np.asarray(f))**2))\n",
    "\n",
    "print(f'Test RMSE (direct, one-step):  Ridge={rmse(y_te.values, ridge_pred):,.0f}  '\n",
    "      f'LASSO={rmse(y_te.values, lasso_pred):,.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7-bv-illustration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 7. Bias-Variance Illustration (Synthetic) ─────────────────────────────────\n",
    "# True function: y = sin(2*pi*x) + epsilon;  x in [0,1]\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "n_train, n_test = 30, 300\n",
    "sigma_true      = 0.25\n",
    "\n",
    "x_train = np.sort(rng.uniform(0, 1, n_train))\n",
    "y_train = np.sin(2 * np.pi * x_train) + rng.normal(0, sigma_true, n_train)\n",
    "\n",
    "x_test  = np.linspace(0, 1, n_test)\n",
    "y_test  = np.sin(2 * np.pi * x_test) + rng.normal(0, sigma_true, n_test)\n",
    "\n",
    "degrees = [1, 2, 3, 5, 8, 12, 15]\n",
    "train_rmse_bv = []\n",
    "test_rmse_bv  = []\n",
    "\n",
    "for deg in degrees:\n",
    "    poly = PolynomialFeatures(degree=deg, include_bias=True)\n",
    "    X_tr_bv = poly.fit_transform(x_train.reshape(-1, 1))\n",
    "    X_te_bv = poly.transform(x_test.reshape(-1, 1))\n",
    "\n",
    "    # Fit with Ridge (tiny alpha) to avoid singular matrix at high degree\n",
    "    model = Ridge(alpha=1e-6).fit(X_tr_bv, y_train)\n",
    "\n",
    "    y_pred_tr = model.predict(X_tr_bv)\n",
    "    y_pred_te = model.predict(X_te_bv)\n",
    "\n",
    "    train_rmse_bv.append(rmse(y_train, y_pred_tr))\n",
    "    test_rmse_bv.append(rmse(y_test, y_pred_te))\n",
    "\n",
    "# Optimal degree: where test RMSE is minimised\n",
    "optimal_idx = int(np.argmin(test_rmse_bv))\n",
    "optimal_deg = degrees[optimal_idx]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(11, 4))\n",
    "\n",
    "# Left: train error\n",
    "axes[0].plot(degrees, train_rmse_bv, color=UNO_BLUE,\n",
    "             lw=2, marker='o', markersize=6)\n",
    "axes[0].axhline(sigma_true, color=UNO_RED, ls=':', lw=1.5,\n",
    "                label=f'Irreducible $\\\\sigma = {sigma_true}$')\n",
    "axes[0].axvline(optimal_deg, color=UNO_GRAY, ls='--', lw=1.2)\n",
    "axes[0].set_xlabel('Polynomial degree (complexity)')\n",
    "axes[0].set_ylabel('RMSE')\n",
    "axes[0].set_title('Train RMSE', fontweight='bold')\n",
    "axes[0].legend()\n",
    "\n",
    "# Right: test error\n",
    "axes[1].plot(degrees, test_rmse_bv, color=UNO_RED,\n",
    "             lw=2, marker='o', markersize=6)\n",
    "axes[1].axhline(sigma_true, color=UNO_RED, ls=':', lw=1.5,\n",
    "                label=f'Irreducible $\\\\sigma = {sigma_true}$')\n",
    "axes[1].axvline(optimal_deg, color=UNO_GRAY, ls='--', lw=1.2,\n",
    "                label=f'Optimal degree = {optimal_deg}')\n",
    "axes[1].set_xlabel('Polynomial degree (complexity)')\n",
    "axes[1].set_ylabel('RMSE')\n",
    "axes[1].set_title('Test RMSE (U-shape = bias-variance tradeoff)', fontweight='bold')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.suptitle('Bias-Variance Illustration — $y = \\\\sin(2\\\\pi x) + \\\\varepsilon$',\n",
    "             fontsize=11, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES / 'lecture07_bias_variance.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f'Train RMSE always decreases with complexity: '\n",
    "      f'{train_rmse_bv[0]:.3f} → {train_rmse_bv[-1]:.4f}')\n",
    "print(f'Test RMSE is U-shaped: minimum at degree {optimal_deg} '\n",
    "      f'(RMSE = {test_rmse_bv[optimal_idx]:.3f})')\n",
    "print(f'Degree-15 overfit: train={train_rmse_bv[-1]:.4f}, '\n",
    "      f'test={test_rmse_bv[-1]:.3f} ({test_rmse_bv[-1]/test_rmse_bv[optimal_idx]:.1f}x worse)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 8. Model Comparison ───────────────────────────────────────────────────────\n",
    "# Walk-forward evaluation for Ridge and LASSO (using feature matrix)\n",
    "# Direct multi-step: one model per step is expensive;\n",
    "# here we use a simple iterated 1-step approach on the feature matrix.\n",
    "\n",
    "def ml_walk_forward(series_in, X_feat, y_feat, model_cls, alpha_star, T0_ml, H):\n",
    "    \"\"\"\n",
    "    Walk-forward evaluation for a penalised linear model on a feature matrix.\n",
    "    Uses direct one-step predictions only (horizon=1 per origin).\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    n = len(X_feat)\n",
    "    for t in range(T0_ml, n - 1):\n",
    "        X_tr_t = X_feat.iloc[:t].values\n",
    "        y_tr_t = y_feat.iloc[:t].values\n",
    "        sc     = StandardScaler().fit(X_tr_t)\n",
    "        model  = model_cls(alpha=alpha_star).fit(sc.transform(X_tr_t), y_tr_t)\n",
    "        pred   = model.predict(sc.transform(X_feat.iloc[[t]].values))[0]\n",
    "        actual = float(y_feat.iloc[t])\n",
    "        records.append({'origin': t, 'horizon': 1,\n",
    "                        'actual': actual, 'forecast': pred,\n",
    "                        'error':  actual - pred})\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "\n",
    "# T0_ml: start walk-forward at 70% of the feature matrix\n",
    "T0_ml = int(len(X_full) * 0.70)\n",
    "\n",
    "print('Ridge walk-forward...', end=' ')\n",
    "wf_ridge = ml_walk_forward(\n",
    "    series, X_full, y_full, Ridge, alpha_ridge_star, T0_ml, H=1)\n",
    "print('done.')\n",
    "\n",
    "print('LASSO walk-forward...', end=' ')\n",
    "wf_lasso = ml_walk_forward(\n",
    "    series, X_full, y_full, Lasso, alpha_lasso_star, T0_ml, H=1)\n",
    "print('done.')\n",
    "\n",
    "# Align naive and SARIMA to T0_ml origins\n",
    "n_feat = len(X_full)\n",
    "feat_offset = len(series) - n_feat   # rows dropped by NaN from lags\n",
    "\n",
    "def rmse_series(df):\n",
    "    d = df.dropna()\n",
    "    return rmse(d['actual'], d['forecast'])\n",
    "\n",
    "# Summary table\n",
    "summary_rows = [\n",
    "    {'Model': 'Seasonal Naive',\n",
    "     'RMSE(h=1)': naive_profile[1],   'RMSE(h=3)': naive_profile[3],\n",
    "     'RMSE(h=12)': naive_profile[12]},\n",
    "    {'Model': 'SARIMA(1,1,1)(0,1,1)',\n",
    "     'RMSE(h=1)': sarima_profile[1],  'RMSE(h=3)': sarima_profile[3],\n",
    "     'RMSE(h=12)': sarima_profile[12]},\n",
    "    {'Model': f'Ridge (α={alpha_ridge_star:.1f})',\n",
    "     'RMSE(h=1)': rmse_series(wf_ridge),\n",
    "     'RMSE(h=3)': None, 'RMSE(h=12)': None},\n",
    "    {'Model': f'LASSO (α={alpha_lasso_star:.1f})',\n",
    "     'RMSE(h=1)': rmse_series(wf_lasso),\n",
    "     'RMSE(h=3)': None, 'RMSE(h=12)': None},\n",
    "]\n",
    "summary_df = pd.DataFrame(summary_rows).set_index('Model').round(0)\n",
    "print('\\nModel comparison (RMSE):')\n",
    "print(summary_df.to_string())\n",
    "\n",
    "# Horizon profile plot (Naive + SARIMA only; Ridge/LASSO are h=1)\n",
    "fig, ax = plt.subplots(figsize=(9, 4))\n",
    "ax.plot(naive_profile.index,  naive_profile.values,\n",
    "        color=UNO_GRAY, lw=1.5, ls='--', marker='o', markersize=4,\n",
    "        label='Seasonal Naive')\n",
    "ax.plot(sarima_profile.index, sarima_profile.values,\n",
    "        color=UNO_BLUE, lw=1.8, marker='o', markersize=4,\n",
    "        label='SARIMA(1,1,1)(0,1,1)')\n",
    "\n",
    "# Mark Ridge and LASSO at h=1\n",
    "ax.scatter([1], [rmse_series(wf_ridge)], color=UNO_GREEN, s=80, zorder=5,\n",
    "           label=f'Ridge (α={alpha_ridge_star:.1f}), h=1')\n",
    "ax.scatter([1], [rmse_series(wf_lasso)], color=UNO_RED,   s=80,\n",
    "           marker='D', zorder=5,\n",
    "           label=f'LASSO (α={alpha_lasso_star:.1f}), h=1')\n",
    "\n",
    "ax.set_xlabel('Forecast horizon $h$')\n",
    "ax.set_ylabel('RMSE')\n",
    "ax.set_title('Horizon RMSE Profile — All Models', fontweight='bold')\n",
    "ax.legend()\n",
    "ax.yaxis.set_major_formatter(\n",
    "    mticker.FuncFormatter(lambda x, _: f'{x:,.0f}'))\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES / 'lecture07_horizon_profile.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Horizon profile saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9-lasso-path",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 9. LASSO Coefficient Path ─────────────────────────────────────────────────\n",
    "# sklearn lasso_path works on the training data\n",
    "X_path = X_tv_s.copy()\n",
    "y_path = y_trainval.copy()\n",
    "\n",
    "# Standardise (lasso_path expects standardised features for proper path)\n",
    "alphas_path, coefs_path, _ = lasso_path(\n",
    "    X_path, y_path,\n",
    "    alphas=np.logspace(np.log10(alpha_lasso_star * 0.01),\n",
    "                       np.log10(alpha_lasso_star * 100), 60),\n",
    "    max_iter=5000\n",
    ")\n",
    "\n",
    "feature_names = list(X_full.columns)\n",
    "n_features    = len(feature_names)\n",
    "\n",
    "# Choose colours: lag features blue, rolling green, calendar/trend gray\n",
    "def feat_color(name):\n",
    "    if name.startswith('lag'):         return UNO_BLUE\n",
    "    if name.startswith('roll_mean'):   return UNO_GREEN\n",
    "    if name.startswith('roll_std'):    return UNO_ORANGE\n",
    "    return UNO_GRAY\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4.5))\n",
    "\n",
    "# Left: LASSO path\n",
    "ax = axes[0]\n",
    "for j, name in enumerate(feature_names):\n",
    "    ax.plot(np.log10(alphas_path), coefs_path[j],\n",
    "            color=feat_color(name), lw=1.2, alpha=0.8)\n",
    "ax.axvline(np.log10(alpha_lasso_star), color='black', ls='--', lw=1.2,\n",
    "           label=f'$\\\\alpha^* = {alpha_lasso_star:.2f}$')\n",
    "ax.axhline(0, color='black', lw=0.5)\n",
    "ax.set_xlabel('$\\\\log_{10}(\\\\alpha)$')\n",
    "ax.set_ylabel('Coefficient $\\\\hat{\\\\beta}_j$')\n",
    "ax.set_title('LASSO Coefficient Path', fontweight='bold')\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "# Add feature label for the last non-zero coefficient at alpha_star\n",
    "coef_at_star = coefs_path[:, np.searchsorted(alphas_path[::-1],\n",
    "                                              alpha_lasso_star)][::-1]\n",
    "for j, (name, coef) in enumerate(zip(feature_names, coef_at_star)):\n",
    "    if abs(coef) > 1.0:\n",
    "        ax.annotate(name,\n",
    "                    xy=(np.log10(alpha_lasso_star), coef),\n",
    "                    xytext=(np.log10(alpha_lasso_star) + 0.3, coef),\n",
    "                    fontsize=6, color=feat_color(name),\n",
    "                    arrowprops=dict(arrowstyle='->', lw=0.6,\n",
    "                                   color=feat_color(name)))\n",
    "\n",
    "# Right: Ridge path (smooth shrinkage for comparison)\n",
    "alphas_ridge_path = np.logspace(-1, 5, 60)\n",
    "coefs_ridge_path  = np.array([\n",
    "    Ridge(alpha=a).fit(X_tv_s, y_trainval).coef_\n",
    "    for a in alphas_ridge_path\n",
    "]).T\n",
    "\n",
    "ax2 = axes[1]\n",
    "for j, name in enumerate(feature_names):\n",
    "    ax2.plot(np.log10(alphas_ridge_path), coefs_ridge_path[j],\n",
    "             color=feat_color(name), lw=1.2, alpha=0.8)\n",
    "ax2.axvline(np.log10(alpha_ridge_star), color='black', ls='--', lw=1.2,\n",
    "            label=f'$\\\\alpha^* = {alpha_ridge_star:.2f}$')\n",
    "ax2.axhline(0, color='black', lw=0.5)\n",
    "ax2.set_xlabel('$\\\\log_{10}(\\\\alpha)$')\n",
    "ax2.set_ylabel('Coefficient $\\\\hat{\\\\beta}_j$')\n",
    "ax2.set_title('Ridge Coefficient Path (smooth shrinkage)', fontweight='bold')\n",
    "ax2.legend(fontsize=8)\n",
    "\n",
    "plt.suptitle('LASSO vs. Ridge Shrinkage: Feature Selection vs. Smooth Shrinkage',\n",
    "             fontsize=11, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES / 'lecture07_lasso_path.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Active features at alpha_star\n",
    "final_lasso_coef = Lasso(alpha=alpha_lasso_star, max_iter=10000)\\\n",
    "    .fit(X_tv_s, y_trainval).coef_\n",
    "active = [(name, coef) for name, coef\n",
    "          in zip(feature_names, final_lasso_coef) if coef != 0]\n",
    "print(f'LASSO active features at alpha*={alpha_lasso_star:.2f} '\n",
    "      f'({len(active)}/{len(feature_names)}):'\n",
    ")\n",
    "for name, coef in sorted(active, key=lambda x: abs(x[1]), reverse=True):\n",
    "    print(f'  {name:<18s}  β = {coef:+.2f}')\n",
    "\n",
    "print('\\n── Discussion questions ──')\n",
    "questions = [\n",
    "    '1. Which LASSO coefficient hits zero first as alpha increases?',\n",
    "    '   What does that tell you about this feature\\'s predictive value?',\n",
    "    '2. Ridge test RMSE vs. LASSO: which is lower? Does LASSO still have value?',\n",
    "    '3. Increase TimeSeriesSplit n_splits from 5 to 10. Does alpha* change?',\n",
    "    '4. The polynomial degree-15 model has near-zero train RMSE.',\n",
    "    '   Would you deploy it? Why?',\n",
    "    '5. Which feature has the highest Ridge coefficient?',\n",
    "    '   Is that consistent with what SARIMA selected implicitly?',\n",
    "]\n",
    "for q in questions:\n",
    "    print(q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
