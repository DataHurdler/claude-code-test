{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-md-0",
   "metadata": {},
   "source": [
    "# Lab 10: Neural Networks for Time Series — LSTM\n",
    "\n",
    "**BSAD 8310: Business Forecasting | University of Nebraska at Omaha**\n",
    "\n",
    "## Objectives\n",
    "\n",
    "1. Reshape the RSXFS lag-feature matrix into 3-D LSTM sequences\n",
    "2. Build and train a stacked LSTM with early stopping\n",
    "3. Diagnose seed variance (5 seeds, box plot)\n",
    "4. Run a lookback ablation (T ∈ {12, 18, 24, 36})\n",
    "5. Compare LSTM against SARIMA and XGBoost baselines\n",
    "\n",
    "## Packages Required\n",
    "```\n",
    "numpy, pandas, matplotlib, scikit-learn, statsmodels, xgboost\n",
    "tensorflow>=2.12  (or tensorflow-cpu)\n",
    "pandas_datareader (optional — FRED data)\n",
    "```\n",
    "\n",
    "> **Note:** If TensorFlow is not installed, the LSTM sections are skipped and only the baselines run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Section 1: Setup\n",
    "# =============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "    tf.random.set_seed(42)\n",
    "    TF_AVAILABLE = True\n",
    "    print(f'TensorFlow {tf.__version__} available.')\n",
    "except ImportError:\n",
    "    TF_AVAILABLE = False\n",
    "    print('TensorFlow not installed — LSTM sections will be skipped.')\n",
    "\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    XGB_AVAILABLE = False\n",
    "    print('xgboost not installed — XGBoost baseline will be skipped.')\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# UNO color palette\n",
    "UNO = {\n",
    "    'blue':      '#005CA9',\n",
    "    'red':       '#E41C38',\n",
    "    'gray':      '#525252',\n",
    "    'green':     '#15803d',\n",
    "    'lightblue': '#cce0f5',\n",
    "    'lightgray': '#e5e5e5',\n",
    "}\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'figure.dpi':        150,\n",
    "    'axes.spines.top':   False,\n",
    "    'axes.spines.right': False,\n",
    "    'font.size':         11,\n",
    "    'axes.titlesize':    13,\n",
    "})\n",
    "\n",
    "FIGURE_DIR = '../Figures'\n",
    "import os; os.makedirs(FIGURE_DIR, exist_ok=True)\n",
    "print('Setup complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Section 2: Load Data and Build Feature Matrix\n",
    "# =============================================================================\n",
    "# Identical feature engineering to Labs 08 and 09 for fair comparison.\n",
    "\n",
    "# --- Load RSXFS ---\n",
    "try:\n",
    "    import pandas_datareader.data as web\n",
    "    raw = web.DataReader('RSXFS', 'fred',\n",
    "                         start='1992-01-01', end='2024-12-01')\n",
    "    y_all = raw['RSXFS'].dropna()\n",
    "    y_all.index = pd.to_datetime(y_all.index).to_period('M')\n",
    "    print(f'Loaded FRED RSXFS: {len(y_all)} monthly observations')\n",
    "except Exception:\n",
    "    import statsmodels.api as sm\n",
    "    macro = sm.datasets.macrodata.load_pandas().data\n",
    "    macro.index = pd.period_range('1959Q1', periods=len(macro), freq='Q')\n",
    "    y_all = macro['realgdp']\n",
    "    print('Loaded statsmodels macrodata fallback.')\n",
    "\n",
    "# --- Feature engineering (leakage-free, same as L08/L09) ---\n",
    "def make_features(y, n_lags=12, roll_windows=(3, 6, 12), add_calendar=True):\n",
    "    df = pd.DataFrame({'y': y})\n",
    "    for k in range(1, n_lags + 1):\n",
    "        df[f'lag_{k}'] = y.shift(k)\n",
    "    y_lag1 = y.shift(1)\n",
    "    for w in roll_windows:\n",
    "        df[f'roll_mean_{w}'] = y_lag1.rolling(w).mean()\n",
    "        df[f'roll_std_{w}']  = y_lag1.rolling(w).std()\n",
    "    if add_calendar:\n",
    "        if hasattr(y.index, 'to_timestamp'):\n",
    "            month = y.index.to_timestamp().month\n",
    "        elif hasattr(y.index, 'month'):\n",
    "            month = y.index.month\n",
    "        else:\n",
    "            month = None\n",
    "        if month is not None:\n",
    "            for m in range(2, 13):\n",
    "                df[f'month_{m}'] = (month == m).astype(int)\n",
    "    df.dropna(inplace=True)\n",
    "    return df.drop(columns=['y']), df['y']\n",
    "\n",
    "X_df, y_series = make_features(y_all, n_lags=12, roll_windows=(3, 6, 12))\n",
    "feat_names = X_df.columns.tolist()\n",
    "\n",
    "# Convert to numpy for easier sequence slicing\n",
    "X_arr = X_df.values.astype(np.float32)\n",
    "y_arr = y_series.values.astype(np.float32)\n",
    "\n",
    "n_total  = len(y_arr)\n",
    "n_features = X_arr.shape[1]\n",
    "\n",
    "print(f'Feature matrix: {n_total} × {n_features}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Section 3: Sequence Construction\n",
    "# =============================================================================\n",
    "# LSTM requires 3-D input: (n_samples, T, n_features)\n",
    "# make_sequences stacks T consecutive feature vectors → one sample.\n",
    "\n",
    "T = 24  # lookback window: two full years (recommended for monthly data)\n",
    "\n",
    "def make_sequences(X_arr, y_arr, T):\n",
    "    \"\"\"Stack sliding windows of length T.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_arr : ndarray of shape (n, p)\n",
    "    y_arr : ndarray of shape (n,)\n",
    "    T     : int, lookback window\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Xs : ndarray (n-T, T, p)   — LSTM input\n",
    "    ys : ndarray (n-T,)         — corresponding targets\n",
    "    \"\"\"\n",
    "    Xs, ys = [], []\n",
    "    for i in range(T, len(X_arr)):\n",
    "        Xs.append(X_arr[i - T : i])   # window [i-T, i)\n",
    "        ys.append(y_arr[i])            # target is the observation after the window\n",
    "    return np.array(Xs, dtype=np.float32), np.array(ys, dtype=np.float32)\n",
    "\n",
    "Xs, ys = make_sequences(X_arr, y_arr, T)\n",
    "n_seq = len(ys)\n",
    "\n",
    "# Three-way split — aligned with the sequence indices\n",
    "# Note: first T observations are consumed to form the first sequence\n",
    "n_test = int(0.15 * n_seq)\n",
    "n_val  = int(0.15 * n_seq)\n",
    "n_tr   = n_seq - n_val - n_test\n",
    "\n",
    "Xs_tr, ys_tr = Xs[:n_tr],             ys[:n_tr]\n",
    "Xs_va, ys_va = Xs[n_tr:n_tr+n_val],   ys[n_tr:n_tr+n_val]\n",
    "Xs_te, ys_te = Xs[n_tr+n_val:],       ys[n_tr+n_val:]\n",
    "\n",
    "# Index labels for the test period (for plotting)\n",
    "test_index = y_series.index[T + n_tr + n_val : T + n_tr + n_val + len(ys_te)]\n",
    "\n",
    "print(f'Sequences: {n_seq} | Train: {n_tr} | Val: {n_val} | Test: {len(ys_te)}')\n",
    "print(f'Input shape: {Xs_tr.shape}  (n_train, T={T}, p={n_features})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Section 4: LSTM — Build, Train, and Diagnose\n",
    "# =============================================================================\n",
    "\n",
    "if TF_AVAILABLE:\n",
    "    def build_lstm(T, n_features, units1=64, units2=32, dropout=0.2,\n",
    "                   recurrent_dropout=0.1, lr=0.001):\n",
    "        \"\"\"Stacked two-layer LSTM for one-step-ahead regression.\"\"\"\n",
    "        model = keras.Sequential([\n",
    "            layers.LSTM(\n",
    "                units1,\n",
    "                return_sequences=True,\n",
    "                dropout=dropout,\n",
    "                recurrent_dropout=recurrent_dropout,\n",
    "                input_shape=(T, n_features)\n",
    "            ),\n",
    "            layers.LSTM(\n",
    "                units2,\n",
    "                dropout=dropout,\n",
    "                recurrent_dropout=recurrent_dropout\n",
    "            ),\n",
    "            layers.Dense(1)\n",
    "        ])\n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "            loss='mse'\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    # --- Train one LSTM (seed=42) ---\n",
    "    tf.random.set_seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    lstm_model = build_lstm(T, n_features)\n",
    "    lstm_model.summary()\n",
    "\n",
    "    cb = keras.callbacks.EarlyStopping(\n",
    "        patience=20, restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    history = lstm_model.fit(\n",
    "        Xs_tr, ys_tr,\n",
    "        validation_data=(Xs_va, ys_va),\n",
    "        epochs=200,\n",
    "        batch_size=16,\n",
    "        callbacks=[cb],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # --- Training / validation loss plot ---\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    ax.plot(history.history['loss'], color=UNO['blue'], lw=1.8,\n",
    "            label='Train loss (MSE)')\n",
    "    ax.plot(history.history['val_loss'], color=UNO['red'], lw=1.8,\n",
    "            ls='--', label='Val loss (MSE)')\n",
    "    stopped = cb.stopped_epoch if cb.stopped_epoch > 0 else len(history.history['loss'])\n",
    "    best_ep = max(0, stopped - cb.patience)\n",
    "    ax.axvline(best_ep, color=UNO['gray'], ls=':', lw=1.2,\n",
    "               label=f'Best epoch ({best_ep})')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('MSE')\n",
    "    ax.set_title('LSTM Training vs Validation Loss')\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{FIGURE_DIR}/lecture10_training_loss.png',\n",
    "                dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f'Saved lecture10_training_loss.png')\n",
    "\n",
    "    # --- Test RMSE (seed=42) ---\n",
    "    y_pred_lstm_s42 = lstm_model.predict(Xs_te).flatten()\n",
    "    rmse_lstm_s42 = np.sqrt(mean_squared_error(ys_te, y_pred_lstm_s42))\n",
    "    print(f'Test RMSE (LSTM, seed=42): {rmse_lstm_s42:.1f}')\n",
    "\nelse:\n",
    "    print('TensorFlow not available — skipping LSTM training.')\n",
    "    y_pred_lstm_s42 = None\n",
    "    rmse_lstm_s42   = float('nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Section 5: Seed Variance — 5 Random Seeds\n",
    "# =============================================================================\n",
    "# Neural networks are sensitive to initialization. Always report median RMSE\n",
    "# across multiple seeds, not just the best run.\n",
    "\n",
    "if TF_AVAILABLE:\n",
    "    SEEDS = [0, 7, 13, 42, 99]\n",
    "    seed_rmses = []\n",
    "\n",
    "    for s in SEEDS:\n",
    "        tf.random.set_seed(s)\n",
    "        np.random.seed(s)\n",
    "        m = build_lstm(T, n_features)\n",
    "        cb_s = keras.callbacks.EarlyStopping(\n",
    "            patience=20, restore_best_weights=True, verbose=0\n",
    "        )\n",
    "        m.fit(\n",
    "            Xs_tr, ys_tr,\n",
    "            validation_data=(Xs_va, ys_va),\n",
    "            epochs=200, batch_size=16,\n",
    "            callbacks=[cb_s], verbose=0\n",
    "        )\n",
    "        pred_s = m.predict(Xs_te, verbose=0).flatten()\n",
    "        r = np.sqrt(mean_squared_error(ys_te, pred_s))\n",
    "        seed_rmses.append(r)\n",
    "        print(f'  Seed {s:2d}: RMSE = {r:.1f}')\n",
    "\n",
    "    median_rmse = np.median(seed_rmses)\n",
    "    print(f'\\nMedian RMSE (5 seeds): {median_rmse:.1f}')\n",
    "\n",
    "    # --- Box plot ---\n",
    "    fig, ax = plt.subplots(figsize=(5, 4))\n",
    "    bp = ax.boxplot([seed_rmses], patch_artist=True,\n",
    "                    medianprops=dict(color=UNO['red'], lw=2),\n",
    "                    boxprops=dict(facecolor=UNO['lightblue']))\n",
    "    ax.scatter([1] * len(seed_rmses), seed_rmses,\n",
    "               color=UNO['blue'], zorder=3, s=50, label='Individual seeds')\n",
    "    ax.set_xticks([1])\n",
    "    ax.set_xticklabels(['LSTM\\n(5 seeds)'])\n",
    "    ax.set_ylabel('Test RMSE')\n",
    "    ax.set_title('LSTM Seed Variance\\n(n=300 monthly obs.)')\n",
    "    ax.legend(fontsize=9)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{FIGURE_DIR}/lecture10_seed_variance.png',\n",
    "                dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print('Saved lecture10_seed_variance.png')\n",
    "\nelse:\n",
    "    print('TensorFlow not available — skipping seed variance analysis.')\n",
    "    seed_rmses   = []\n",
    "    median_rmse  = float('nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Section 6: Lookback Ablation — T ∈ {12, 18, 24, 36}\n",
    "# =============================================================================\n",
    "# How sensitive is LSTM performance to the choice of lookback window T?\n",
    "\n",
    "if TF_AVAILABLE:\n",
    "    T_values = [12, 18, 24, 36]\n",
    "    ablation_rmse = {}\n",
    "\n",
    "    for T_ab in T_values:\n",
    "        Xs_ab, ys_ab = make_sequences(X_arr, y_arr, T_ab)\n",
    "        n_ab   = len(ys_ab)\n",
    "        n_te   = int(0.15 * n_ab)\n",
    "        n_va   = int(0.15 * n_ab)\n",
    "        n_tr_ab = n_ab - n_va - n_te\n",
    "\n",
    "        Xs_tr_ab = Xs_ab[:n_tr_ab]\n",
    "        ys_tr_ab = ys_ab[:n_tr_ab]\n",
    "        Xs_va_ab = Xs_ab[n_tr_ab:n_tr_ab+n_va]\n",
    "        ys_va_ab = ys_ab[n_tr_ab:n_tr_ab+n_va]\n",
    "        Xs_te_ab = Xs_ab[n_tr_ab+n_va:]\n",
    "        ys_te_ab = ys_ab[n_tr_ab+n_va:]\n",
    "\n",
    "        # Use seed=42 for each T\n",
    "        tf.random.set_seed(42)\n",
    "        np.random.seed(42)\n",
    "        m_ab = build_lstm(T_ab, n_features)\n",
    "        cb_ab = keras.callbacks.EarlyStopping(\n",
    "            patience=20, restore_best_weights=True, verbose=0\n",
    "        )\n",
    "        m_ab.fit(\n",
    "            Xs_tr_ab, ys_tr_ab,\n",
    "            validation_data=(Xs_va_ab, ys_va_ab),\n",
    "            epochs=200, batch_size=16,\n",
    "            callbacks=[cb_ab], verbose=0\n",
    "        )\n",
    "        pred_ab = m_ab.predict(Xs_te_ab, verbose=0).flatten()\n",
    "        r_ab = np.sqrt(mean_squared_error(ys_te_ab, pred_ab))\n",
    "        ablation_rmse[T_ab] = r_ab\n",
    "        print(f'  T={T_ab:2d}: Test RMSE = {r_ab:.1f}')\n",
    "\n",
    "    # --- Lookback ablation curve ---\n",
    "    fig, ax = plt.subplots(figsize=(7, 4))\n",
    "    ax.plot(list(ablation_rmse.keys()), list(ablation_rmse.values()),\n",
    "            'o-', color=UNO['blue'], lw=2, ms=7)\n",
    "    ax.axvline(24, color=UNO['gray'], ls='--', lw=1.2,\n",
    "               label='T=24 (recommended)')\n",
    "    ax.set_xlabel('Lookback window T (months)')\n",
    "    ax.set_ylabel('Test RMSE')\n",
    "    ax.set_title('LSTM Lookback Ablation (seed=42)')\n",
    "    ax.set_xticks(T_values)\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{FIGURE_DIR}/lecture10_lookback_ablation.png',\n",
    "                dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print('Saved lecture10_lookback_ablation.png')\n",
    "\nelse:\n",
    "    print('TensorFlow not available — skipping lookback ablation.')\n",
    "    ablation_rmse = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# Section 7: Baselines — SARIMA and XGBoost\n# =============================================================================\n# Using the same train/val/test split as Labs 08 and 09 for comparability.\n# XGBoost uses the flat feature matrix (not sequences).\n\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\n\n# --- Flat-feature split (same as L08/L09) ---\nn_flat   = len(y_arr)\nn_te_fl  = int(0.15 * n_flat)\nn_va_fl  = int(0.15 * n_flat)\nn_tr_fl  = n_flat - n_va_fl - n_te_fl\n\nX_train_fl = X_arr[:n_tr_fl]\ny_train_fl = y_arr[:n_tr_fl]\nX_val_fl   = X_arr[n_tr_fl:n_tr_fl+n_va_fl]\ny_val_fl   = y_arr[n_tr_fl:n_tr_fl+n_va_fl]\nX_tv_fl    = X_arr[:n_tr_fl+n_va_fl]\ny_tv_fl    = y_arr[:n_tr_fl+n_va_fl]\nX_test_fl  = X_arr[n_tr_fl+n_va_fl:]\ny_test_fl  = y_arr[n_tr_fl+n_va_fl:]\n\n# Keep index labels for the test period\nflat_test_index = y_series.index[n_tr_fl+n_va_fl : n_tr_fl+n_va_fl+len(y_test_fl)]\n\n# --- SARIMA ---\ny_tv_series = y_series.iloc[:n_tr_fl+n_va_fl]\ntry:\n    sarima_mod = SARIMAX(\n        y_tv_series,\n        order=(1, 1, 1),\n        seasonal_order=(1, 1, 1, 12),\n        enforce_stationarity=False,\n        enforce_invertibility=False\n    )\n    sarima_res  = sarima_mod.fit(disp=False)\n    y_pred_sar  = sarima_res.forecast(len(y_test_fl))\n    rmse_sarima = np.sqrt(mean_squared_error(y_test_fl, y_pred_sar))\n    sarima_ok   = True\n    print(f'SARIMA RMSE: {rmse_sarima:.1f}')\nexcept Exception as e:\n    print(f'SARIMA failed: {e}')\n    sarima_ok   = False\n    y_pred_sar  = pd.Series([y_tv_series.mean()] * len(y_test_fl))\n    rmse_sarima = float('nan')\n\n# --- XGBoost ---\nif XGB_AVAILABLE:\n    dtrain_fl    = xgb.DMatrix(X_train_fl, label=y_train_fl)\n    dval_fl      = xgb.DMatrix(X_val_fl,   label=y_val_fl)\n    dtrainval_fl = xgb.DMatrix(X_tv_fl,    label=y_tv_fl)\n    dtest_fl     = xgb.DMatrix(X_test_fl,  label=y_test_fl)\n\n    xgb_params = {\n        'learning_rate':    0.05,\n        'max_depth':        4,\n        'subsample':        0.8,\n        'colsample_bytree': 0.8,\n        'reg_lambda':       1.0,\n        'objective':        'reg:squarederror',\n        'eval_metric':      'rmse',\n        'verbosity':        0,   # suppress output (verbose_eval deprecated in XGBoost 2.0)\n        'seed':             42,\n    }\n    xgb_tmp = xgb.train(\n        xgb_params, dtrain_fl,\n        num_boost_round=2000,\n        evals=[(dval_fl, 'val')],\n        early_stopping_rounds=50\n    )\n    best_rounds = xgb_tmp.best_iteration\n    xgb_final = xgb.train(\n        xgb_params, dtrainval_fl,\n        num_boost_round=best_rounds\n    )\n    y_pred_xgb = xgb_final.predict(dtest_fl)\n    rmse_xgb   = np.sqrt(mean_squared_error(y_test_fl, y_pred_xgb))\n    print(f'XGBoost RMSE: {rmse_xgb:.1f}')\nelse:\n    y_pred_xgb = None\n    rmse_xgb   = float('nan')\n    print('XGBoost not available.')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Section 8: Model Comparison Table and Bar Chart\n",
    "# =============================================================================\n",
    "\n",
    "def rmse_fn(actual, predicted):\n",
    "    a = np.asarray(actual)\n",
    "    p = np.asarray(predicted)[:len(a)]\n",
    "    return float(np.sqrt(mean_squared_error(a[:len(p)], p)))\n",
    "\n",
    "def mae_fn(actual, predicted):\n",
    "    a = np.asarray(actual)\n",
    "    p = np.asarray(predicted)[:len(a)]\n",
    "    return float(np.mean(np.abs(a[:len(p)] - p)))\n",
    "\n",
    "rows = [\n",
    "    ('SARIMA(1,1,1)(1,1,1)_12',\n",
    "     rmse_fn(y_test_fl, y_pred_sar),\n",
    "     mae_fn(y_test_fl, y_pred_sar)),\n",
    "]\n",
    "if XGB_AVAILABLE and y_pred_xgb is not None:\n",
    "    rows.append(('XGBoost (early stop)',\n",
    "                 rmse_fn(y_test_fl, y_pred_xgb),\n",
    "                 mae_fn(y_test_fl, y_pred_xgb)))\n",
    "if TF_AVAILABLE and y_pred_lstm_s42 is not None:\n",
    "    rows.append(('LSTM (seed=42)',\n",
    "                 rmse_fn(ys_te, y_pred_lstm_s42),\n",
    "                 mae_fn(ys_te, y_pred_lstm_s42)))\n",
    "if seed_rmses:\n",
    "    rows.append(('LSTM (median, 5 seeds)', float(np.median(seed_rmses)), float('nan')))\n",
    "\n",
    "results = pd.DataFrame(rows, columns=['Model', 'RMSE', 'MAE'])\n",
    "results['RMSE'] = results['RMSE'].round(1)\n",
    "results['MAE']  = results['MAE'].round(1)\n",
    "\n",
    "print('\\n=== Test-Set Comparison ===')\n",
    "print(results.to_string(index=False))\n",
    "\n",
    "# --- Bar chart ---\n",
    "plot_rows = results[results['RMSE'].notna() & (results['RMSE'] > 0)]\n",
    "colors = [UNO['gray'], UNO['red'], UNO['blue'], UNO['blue']]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "bars = ax.barh(\n",
    "    plot_rows['Model'][::-1],\n",
    "    plot_rows['RMSE'][::-1],\n",
    "    color=colors[:len(plot_rows)][::-1],\n",
    "    edgecolor='white'\n",
    ")\n",
    "for bar, rmse_v in zip(bars, plot_rows['RMSE'][::-1]):\n",
    "    ax.text(bar.get_width() + 5, bar.get_y() + bar.get_height() / 2,\n",
    "            f'{rmse_v:,.0f}', va='center', fontsize=9, color=UNO['gray'])\n",
    "ax.set_xlabel('Test RMSE (lower is better)')\n",
    "ax.set_title('Model Comparison: L01–L10 (Direct 1-step forecasts)')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{FIGURE_DIR}/lecture10_model_comparison.png',\n",
    "            dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Saved lecture10_model_comparison.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Section 9: Forecast Comparison Plot\n",
    "# =============================================================================\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(11, 4))\n",
    "\n",
    "# History context (last 24 months of trainval)\n",
    "context = y_series.iloc[n_tr_fl+n_va_fl-24 : n_tr_fl+n_va_fl]\n",
    "ctx_idx = context.index.astype(str)\n",
    "ax.plot(ctx_idx, context.values, color=UNO['lightgray'], lw=1.5,\n",
    "        label='History')\n",
    "\n",
    "# Actuals\n",
    "te_idx = flat_test_index.astype(str)\n",
    "ax.plot(te_idx, y_test_fl, color='black', lw=2, label='Actual', zorder=5)\n",
    "\n",
    "# SARIMA\n",
    "if sarima_ok:\n",
    "    sar_vals = np.asarray(y_pred_sar)[:len(y_test_fl)]\n",
    "    ax.plot(te_idx[:len(sar_vals)], sar_vals,\n",
    "            color=UNO['gray'], lw=1.5, ls='--', label='SARIMA')\n",
    "\n",
    "# XGBoost\n",
    "if XGB_AVAILABLE and y_pred_xgb is not None:\n",
    "    ax.plot(te_idx, y_pred_xgb,\n",
    "            color=UNO['red'], lw=1.8, ls='-.', label='XGBoost')\n",
    "\n",
    "# LSTM (seed=42)\n",
    "if TF_AVAILABLE and y_pred_lstm_s42 is not None:\n",
    "    # Align LSTM predictions to flat test index\n",
    "    lstm_plot = y_pred_lstm_s42[:len(te_idx)]\n",
    "    ax.plot(te_idx[:len(lstm_plot)], lstm_plot,\n",
    "            color=UNO['blue'], lw=2, label='LSTM (seed=42)')\n",
    "\n",
    "ax.set_title('Forecast Comparison: SARIMA vs XGBoost vs LSTM (Test Set)')\n",
    "ax.set_xlabel('Period')\n",
    "ax.set_ylabel('Retail Sales (Millions USD)')\n",
    "ax.legend(loc='upper left', fontsize=9)\n",
    "ax.xaxis.set_major_locator(mticker.MaxNLocator(8))\n",
    "plt.xticks(rotation=30)\n",
    "\n",
    "# RMSE annotation\n",
    "annots = []\n",
    "if sarima_ok:\n",
    "    annots.append(('SARIMA', UNO['gray'],\n",
    "                   rmse_fn(y_test_fl, y_pred_sar)))\n",
    "if XGB_AVAILABLE and y_pred_xgb is not None:\n",
    "    annots.append(('XGBoost', UNO['red'],\n",
    "                   rmse_fn(y_test_fl, y_pred_xgb)))\n",
    "if TF_AVAILABLE and y_pred_lstm_s42 is not None:\n",
    "    annots.append(('LSTM', UNO['blue'],\n",
    "                   rmse_fn(ys_te, y_pred_lstm_s42)))\n",
    "\n",
    "for i, (label, color, r_val) in enumerate(annots):\n",
    "    ax.annotate(f'{label}: RMSE={r_val:,.0f}',\n",
    "                xy=(0.99, 0.97 - i * 0.09),\n",
    "                xycoords='axes fraction',\n",
    "                ha='right', va='top', fontsize=9, color=color)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{FIGURE_DIR}/lecture10_forecast_comparison.png',\n",
    "            dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Saved lecture10_forecast_comparison.png')\n",
    "\n",
    "print('\\n=== Final Results ===')\n",
    "print(results.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}