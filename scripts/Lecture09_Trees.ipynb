{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 09: Tree-Based Methods — Random Forests and XGBoost\n",
    "\n",
    "**BSAD 8310: Business Forecasting | University of Nebraska at Omaha**\n",
    "\n",
    "## Objectives\n",
    "\n",
    "1. Build the lag-feature matrix for RSXFS retail sales (same as Lab 08)\n",
    "2. Fit a Random Forest with TimeSeriesSplit hyperparameter tuning\n",
    "3. Fit XGBoost with early stopping on the validation set\n",
    "4. Visualize feature importance (impurity, permutation, XGBoost gain)\n",
    "5. Compare RF and XGBoost against SARIMA and Elastic Net (L08 baseline)\n",
    "\n",
    "## Packages Required\n",
    "```\n",
    "numpy, pandas, matplotlib, scikit-learn, xgboost, statsmodels\n",
    "pandas_datareader (optional — FRED data)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Section 1: Setup\n",
    "# =============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.model_selection import (\n",
    "    TimeSeriesSplit, RandomizedSearchCV, cross_val_score\n",
    ")\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print('xgboost not installed — XGBoost section will be skipped.')\n",
    "    XGB_AVAILABLE = False\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# UNO color palette\n",
    "UNO = {\n",
    "    'blue':       '#005CA9',\n",
    "    'red':        '#E41C38',\n",
    "    'gray':       '#525252',\n",
    "    'green':      '#15803d',\n",
    "    'lightblue':  '#cce0f5',\n",
    "    'lightgray':  '#e5e5e5',\n",
    "}\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'figure.dpi':        150,\n",
    "    'axes.spines.top':   False,\n",
    "    'axes.spines.right': False,\n",
    "    'font.size':         11,\n",
    "    'axes.titlesize':    13,\n",
    "})\n",
    "\n",
    "FIGURE_DIR = '../Figures'\n",
    "import os; os.makedirs(FIGURE_DIR, exist_ok=True)\n",
    "print('Setup complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Section 2: Load Data and Build Feature Matrix\n",
    "# =============================================================================\n",
    "# Identical feature engineering to Lab 08 for fair comparison.\n",
    "\n",
    "# --- Load data ---\n",
    "try:\n",
    "    import pandas_datareader.data as web\n",
    "    raw = web.DataReader('RSXFS', 'fred',\n",
    "                         start='1992-01-01', end='2024-12-01')\n",
    "    y_all = raw['RSXFS'].dropna()\n",
    "    y_all.index = pd.to_datetime(y_all.index).to_period('M')\n",
    "    print(f'Loaded FRED RSXFS: {len(y_all)} monthly observations')\n",
    "except Exception:\n",
    "    import statsmodels.api as sm\n",
    "    macro = sm.datasets.macrodata.load_pandas().data\n",
    "    macro.index = pd.period_range('1959Q1', periods=len(macro), freq='Q')\n",
    "    y_all = macro['realgdp']\n",
    "    print('Loaded statsmodels macrodata fallback.')\n",
    "\n",
    "# --- Feature engineering (leakage-free) ---\n",
    "def make_features(y, n_lags=12, roll_windows=(3, 6, 12), add_calendar=True):\n",
    "    df = pd.DataFrame({'y': y})\n",
    "    for k in range(1, n_lags + 1):\n",
    "        df[f'lag_{k}'] = y.shift(k)\n",
    "    y_lag1 = y.shift(1)\n",
    "    for w in roll_windows:\n",
    "        df[f'roll_mean_{w}'] = y_lag1.rolling(w).mean()\n",
    "        df[f'roll_std_{w}']  = y_lag1.rolling(w).std()\n",
    "    if add_calendar:\n",
    "        if hasattr(y.index, 'to_timestamp'):\n",
    "            month = y.index.to_timestamp().month\n",
    "        elif hasattr(y.index, 'month'):\n",
    "            month = y.index.month\n",
    "        else:\n",
    "            month = None\n",
    "        if month is not None:\n",
    "            for m in range(2, 13):\n",
    "                df[f'month_{m}'] = (month == m).astype(int)\n",
    "    df.dropna(inplace=True)\n",
    "    return df.drop(columns=['y']), df['y']\n",
    "\n",
    "X, y = make_features(y_all, n_lags=12, roll_windows=(3, 6, 12))\n",
    "\n",
    "# --- Three-way split ---\n",
    "n = len(y)\n",
    "n_test  = int(0.15 * n)\n",
    "n_val   = int(0.15 * n)\n",
    "n_train = n - n_val - n_test\n",
    "\n",
    "X_train, y_train = X.iloc[:n_train],            y.iloc[:n_train]\n",
    "X_val,   y_val   = X.iloc[n_train:n_train+n_val], y.iloc[n_train:n_train+n_val]\n",
    "X_test,  y_test  = X.iloc[n_train+n_val:],      y.iloc[n_train+n_val:]\n",
    "X_trainval = X.iloc[:n_train+n_val]\n",
    "y_trainval = y.iloc[:n_train+n_val]\n",
    "\n",
    "feat_names = X.columns.tolist()\n",
    "print(f'Features: {len(feat_names)} | Train: {n_train} | Val: {n_val} | Test: {n_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Section 3: Random Forest — Hyperparameter Tuning\n",
    "# =============================================================================\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5, gap=0)\n",
    "\n",
    "rf_param_grid = {\n",
    "    'n_estimators':     [200, 500],\n",
    "    'max_features':     ['sqrt', 0.33, 0.5],\n",
    "    'min_samples_leaf': [1, 3, 5],\n",
    "    'max_depth':        [None, 10, 20],\n",
    "}\n",
    "\n",
    "print('Fitting Random Forest via RandomizedSearchCV (TimeSeriesSplit)...')\n",
    "rf_search = RandomizedSearchCV(\n",
    "    RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "    rf_param_grid,\n",
    "    n_iter=20,\n",
    "    cv=tscv,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "rf_search.fit(X_trainval, y_trainval)\n",
    "\n",
    "best_rf_params = rf_search.best_params_\n",
    "print(f'Best RF params: {best_rf_params}')\n",
    "print(f'CV RMSE:        {-rf_search.best_score_:.1f}')\n",
    "\n",
    "# Final RF fitted on train+val with best params\n",
    "rf_model = rf_search.best_estimator_\n",
    "y_pred_rf = pd.Series(rf_model.predict(X_test), index=y_test.index)\n",
    "\n",
    "rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
    "print(f'Test RMSE (RF): {rmse_rf:.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Section 4: XGBoost — Early Stopping on Validation Set\n",
    "# =============================================================================\n",
    "\n",
    "if XGB_AVAILABLE:\n",
    "    dtrain = xgb.DMatrix(X_train,    label=y_train)\n",
    "    dval   = xgb.DMatrix(X_val,      label=y_val)\n",
    "    dtest  = xgb.DMatrix(X_test,     label=y_test)\n",
    "    dtrainval = xgb.DMatrix(X_trainval, label=y_trainval)\n",
    "\n",
    "    xgb_params = {\n",
    "        'learning_rate':    0.05,\n",
    "        'max_depth':        4,\n",
    "        'subsample':        0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'reg_lambda':       1.0,\n",
    "        'objective':        'reg:squarederror',\n",
    "        'eval_metric':      'rmse',\n",
    "        'seed':             42,\n",
    "    }\n",
    "\n",
    "    print('Training XGBoost with early stopping...')\n",
    "    xgb_model = xgb.train(\n",
    "        xgb_params,\n",
    "        dtrain,\n",
    "        num_boost_round=2000,\n",
    "        evals=[(dval, 'val')],\n",
    "        early_stopping_rounds=50,\n",
    "        verbose_eval=100\n",
    "    )\n",
    "    best_rounds = xgb_model.best_iteration\n",
    "    print(f'Best round: {best_rounds}')\n",
    "\n",
    "    # Refit on full trainval with best_rounds\n",
    "    xgb_final = xgb.train(\n",
    "        xgb_params, dtrainval,\n",
    "        num_boost_round=best_rounds\n",
    "    )\n",
    "    y_pred_xgb = pd.Series(\n",
    "        xgb_final.predict(dtest), index=y_test.index\n",
    "    )\n",
    "    rmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))\n",
    "    print(f'Test RMSE (XGBoost): {rmse_xgb:.1f}')\n",
    "else:\n",
    "    print('Skipping XGBoost (not installed).')\n",
    "    y_pred_xgb = None\n",
    "    rmse_xgb   = float('nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Section 5: Feature Importance — Three Methods\n",
    "# =============================================================================\n",
    "\n",
    "n_top = 12  # show top N features\n",
    "\n",
    "# --- 1. Impurity-based importance (RF default) ---\n",
    "imp_df = pd.DataFrame({\n",
    "    'feature': feat_names,\n",
    "    'impurity': rf_model.feature_importances_\n",
    "}).sort_values('impurity', ascending=False).head(n_top)\n",
    "\n",
    "# --- 2. Permutation importance (unbiased, on val set) ---\n",
    "perm_result = permutation_importance(\n",
    "    rf_model, X_val, y_val,\n",
    "    n_repeats=10, random_state=42, scoring='neg_root_mean_squared_error'\n",
    ")\n",
    "perm_df = pd.DataFrame({\n",
    "    'feature': feat_names,\n",
    "    'perm_mean': -perm_result.importances_mean,\n",
    "    'perm_std':  perm_result.importances_std,\n",
    "}).sort_values('perm_mean', ascending=False).head(n_top)\n",
    "\n",
    "# --- 3. XGBoost gain importance ---\n",
    "if XGB_AVAILABLE:\n",
    "    xgb_imp = xgb_final.get_score(importance_type='gain')\n",
    "    xgb_imp_df = pd.DataFrame([\n",
    "        {'feature': k, 'gain': v} for k, v in xgb_imp.items()\n",
    "    ]).sort_values('gain', ascending=False).head(n_top)\n",
    "\n",
    "# --- Plot ---\n",
    "n_panels = 3 if XGB_AVAILABLE else 2\n",
    "fig, axes = plt.subplots(1, n_panels, figsize=(5 * n_panels, 5))\n",
    "\n",
    "# Impurity\n",
    "ax = axes[0]\n",
    "ax.barh(imp_df['feature'][::-1], imp_df['impurity'][::-1],\n",
    "        color=UNO['blue'], edgecolor='white')\n",
    "ax.set_title('RF: Impurity Importance')\n",
    "ax.set_xlabel('Mean decrease in impurity')\n",
    "\n",
    "# Permutation\n",
    "ax = axes[1]\n",
    "ax.barh(perm_df['feature'][::-1], perm_df['perm_mean'][::-1],\n",
    "        xerr=perm_df['perm_std'][::-1],\n",
    "        color=UNO['green'], edgecolor='white', ecolor=UNO['gray'], capsize=3)\n",
    "ax.set_title('RF: Permutation Importance (val)')\n",
    "ax.set_xlabel('Mean RMSE increase when shuffled')\n",
    "\n",
    "# XGBoost gain\n",
    "if XGB_AVAILABLE:\n",
    "    ax = axes[2]\n",
    "    ax.barh(xgb_imp_df['feature'][::-1], xgb_imp_df['gain'][::-1],\n",
    "            color=UNO['red'], edgecolor='white')\n",
    "    ax.set_title('XGBoost: Gain Importance')\n",
    "    ax.set_xlabel('Mean gain per split')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{FIGURE_DIR}/lecture09_feature_importance.png',\n",
    "            dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Saved lecture09_feature_importance.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Section 6: OOB Error vs. n_estimators\n",
    "# =============================================================================\n",
    "# Show how OOB error stabilizes as number of trees grows.\n",
    "\n",
    "n_tree_range = [10, 25, 50, 100, 200, 350, 500]\n",
    "oob_errors = []\n",
    "\n",
    "for n_trees in n_tree_range:\n",
    "    rf_tmp = RandomForestRegressor(\n",
    "        n_estimators=n_trees,\n",
    "        max_features=best_rf_params.get('max_features', 0.33),\n",
    "        min_samples_leaf=best_rf_params.get('min_samples_leaf', 1),\n",
    "        max_depth=best_rf_params.get('max_depth', None),\n",
    "        oob_score=True,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rf_tmp.fit(X_trainval, y_trainval)\n",
    "    oob_pred = rf_tmp.oob_prediction_\n",
    "    oob_rmse = np.sqrt(mean_squared_error(y_trainval, oob_pred))\n",
    "    oob_errors.append(oob_rmse)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.plot(n_tree_range, oob_errors, 'o-',\n",
    "        color=UNO['blue'], lw=2, ms=6)\n",
    "ax.set_xlabel('Number of trees (n_estimators)')\n",
    "ax.set_ylabel('OOB RMSE')\n",
    "ax.set_title('OOB Error Stabilizes with More Trees')\n",
    "ax.axvline(500, color=UNO['gray'], ls='--', lw=1.2, label='n=500 (recommended)')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{FIGURE_DIR}/lecture09_oob_curve.png',\n",
    "            dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Saved lecture09_oob_curve.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Section 7: SARIMA Baseline\n",
    "# =============================================================================\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "try:\n",
    "    sarima_mod = SARIMAX(\n",
    "        y_trainval,\n",
    "        order=(1, 1, 1),\n",
    "        seasonal_order=(1, 1, 1, 12),\n",
    "        enforce_stationarity=False,\n",
    "        enforce_invertibility=False\n",
    "    )\n",
    "    sarima_res = sarima_mod.fit(disp=False)\n",
    "    y_pred_sarima = sarima_res.forecast(len(y_test))\n",
    "    sarima_ok = True\n",
    "    print('SARIMA baseline fit complete.')\n",
    "except Exception as e:\n",
    "    print(f'SARIMA failed: {e}')\n",
    "    sarima_ok = False\n",
    "    y_pred_sarima = pd.Series(\n",
    "        [y_trainval.mean()] * len(y_test), index=y_test.index\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Section 8: Model Comparison\n",
    "# =============================================================================\n",
    "\n",
    "def rmse(actual, predicted):\n",
    "    a = np.asarray(actual)\n",
    "    p = np.asarray(predicted)[:len(a)]\n",
    "    return np.sqrt(mean_squared_error(a[:len(p)], p))\n",
    "\n",
    "def mae(actual, predicted):\n",
    "    a = np.asarray(actual)\n",
    "    p = np.asarray(predicted)[:len(a)]\n",
    "    return np.mean(np.abs(a[:len(p)] - p))\n",
    "\n",
    "rows = [\n",
    "    ('SARIMA(1,1,1)(1,1,1)_12',\n",
    "     rmse(y_test, y_pred_sarima), mae(y_test, y_pred_sarima)),\n",
    "    ('Random Forest',\n",
    "     rmse(y_test, y_pred_rf), mae(y_test, y_pred_rf)),\n",
    "]\n",
    "if XGB_AVAILABLE and y_pred_xgb is not None:\n",
    "    rows.append((\n",
    "        'XGBoost (early stop)',\n",
    "        rmse(y_test, y_pred_xgb), mae(y_test, y_pred_xgb)\n",
    "    ))\n",
    "\n",
    "results = pd.DataFrame(rows, columns=['Model', 'RMSE', 'MAE'])\n",
    "results['RMSE'] = results['RMSE'].round(1)\n",
    "results['MAE']  = results['MAE'].round(1)\n",
    "print('Test-set results:')\n",
    "print(results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Section 9: Forecast Comparison Plot\n",
    "# =============================================================================\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(11, 4))\n",
    "\n",
    "# History context\n",
    "context = y_trainval.iloc[-24:]\n",
    "ax.plot(context.index.astype(str), context.values,\n",
    "        color=UNO['lightgray'], lw=1.5, label='History')\n",
    "\n",
    "# Actuals\n",
    "ax.plot(y_test.index.astype(str), y_test.values,\n",
    "        color='black', lw=2, label='Actual', zorder=5)\n",
    "\n",
    "# Baseline\n",
    "if sarima_ok:\n",
    "    sarima_vals = np.asarray(y_pred_sarima)[:len(y_test)]\n",
    "    ax.plot(y_test.index.astype(str)[:len(sarima_vals)],\n",
    "            sarima_vals, color=UNO['gray'], lw=1.5, ls='--', label='SARIMA')\n",
    "\n",
    "# RF\n",
    "ax.plot(y_test.index.astype(str), y_pred_rf.values,\n",
    "        color=UNO['blue'], lw=2, ls='-.', label='Random Forest')\n",
    "\n",
    "# XGBoost\n",
    "if XGB_AVAILABLE and y_pred_xgb is not None:\n",
    "    ax.plot(y_test.index.astype(str), y_pred_xgb.values,\n",
    "            color=UNO['red'], lw=2, label='XGBoost')\n",
    "\n",
    "ax.set_title('Forecast Comparison: SARIMA vs. Tree-Based Models (Test Set)')\n",
    "ax.set_xlabel('Period')\n",
    "ax.set_ylabel('Retail Sales (Millions USD)')\n",
    "ax.legend(loc='upper left', fontsize=9)\n",
    "ax.xaxis.set_major_locator(mticker.MaxNLocator(8))\n",
    "plt.xticks(rotation=30)\n",
    "\n",
    "# RMSE annotations\n",
    "annot_models = [\n",
    "    ('RF',  UNO['blue'],  y_pred_rf),\n",
    "]\n",
    "if XGB_AVAILABLE and y_pred_xgb is not None:\n",
    "    annot_models.append(('XGB', UNO['red'], y_pred_xgb))\n",
    "\n",
    "for i, (label, color, pred) in enumerate(annot_models):\n",
    "    r = rmse(y_test, pred)\n",
    "    ax.annotate(f'{label}: RMSE={r:,.0f}',\n",
    "                xy=(0.99, 0.97 - i * 0.09),\n",
    "                xycoords='axes fraction',\n",
    "                ha='right', va='top', fontsize=9, color=color)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{FIGURE_DIR}/lecture09_forecast_comparison.png',\n",
    "            dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Saved lecture09_forecast_comparison.png')\n",
    "\n",
    "print('\\n=== Final Results ===')\n",
    "print(results.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
