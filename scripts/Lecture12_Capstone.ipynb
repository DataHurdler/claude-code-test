{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 12: Capstone & Applications\n",
    "**BSAD 8310: Business Forecasting | University of Nebraska at Omaha**\n",
    "\n",
    "This notebook demonstrates the complete BSAD 8310 forecasting toolkit applied to two business case studies:\n",
    "- **RSXFS**: Advance Retail Sales (11-model final leaderboard)\n",
    "- **RESGAS**: U.S. Residential Natural Gas Consumption (second case study)\n",
    "\n",
    "Key outputs: leaderboard charts, DM test heatmap, forecast combination, forecast comparison plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from pathlib import Path\n",
    "\n",
    "# TensorFlow / Keras — optional\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    tf.random.set_seed(42)\n",
    "    TF_AVAILABLE = True\n",
    "    print(f\"TensorFlow {tf.__version__} available\")\n",
    "except ImportError:\n",
    "    TF_AVAILABLE = False\n",
    "    print(\"TensorFlow not available — LSTM results use pre-computed values\")\n",
    "\n",
    "# Statistical modelling\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from sklearn.linear_model import ElasticNetCV, RidgeCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "    XGB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    XGB_AVAILABLE = False\n",
    "    print(\"XGBoost not available — using pre-computed values\")\n",
    "\n",
    "# Random seeds\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# UNO colour palette\n",
    "UNO_BLUE  = '#005CA9'\n",
    "UNO_RED   = '#E41C38'\n",
    "UNO_GREEN = '#15803d'\n",
    "UNO_GRAY  = '#525252'\n",
    "LIGHT_BLUE  = '#DBEAFE'\n",
    "LIGHT_RED   = '#FEE2E2'\n",
    "LIGHT_GREEN = '#DCFCE7'\n",
    "\n",
    "# Matplotlib defaults\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'serif',\n",
    "    'font.size': 11,\n",
    "    'axes.titlesize': 13,\n",
    "    'axes.labelsize': 11,\n",
    "    'axes.spines.top': False,\n",
    "    'axes.spines.right': False,\n",
    "    'figure.dpi': 150,\n",
    "})\n",
    "\n",
    "# Output directory\n",
    "FIG_DIR = Path('../Figures')\n",
    "FIG_DIR.mkdir(exist_ok=True)\n",
    "print('Setup complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Load RSXFS and Build Feature Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Load RSXFS ─────────────────────────────────────────────────────────────\n",
    "# Advance Retail Sales: Retail Trade (RSXFS), monthly, seasonally unadjusted\n",
    "# Source: FRED https://fred.stlouisfed.org/series/RSXFS\n",
    "# Units: Millions of Dollars\n",
    "\n",
    "try:\n",
    "    import pandas_datareader as pdr\n",
    "    rsxfs_raw = pdr.get_data_fred('RSXFS', start='2000-01-01', end='2023-12-31')\n",
    "    rsxfs = rsxfs_raw['RSXFS'].dropna()\n",
    "    print(f\"RSXFS loaded from FRED: {len(rsxfs)} observations ({rsxfs.index[0].date()} – {rsxfs.index[-1].date()})\")\n",
    "except Exception:\n",
    "    # Simulate if FRED unavailable\n",
    "    np.random.seed(SEED)\n",
    "    dates = pd.date_range('2000-01-01', periods=288, freq='MS')\n",
    "    trend = np.linspace(200_000, 550_000, 288)\n",
    "    seasonal = 30_000 * np.sin(2 * np.pi * np.arange(288) / 12 - np.pi / 2)\n",
    "    noise = np.random.normal(0, 8_000, 288)\n",
    "    rsxfs = pd.Series(trend + seasonal + noise, index=dates, name='RSXFS')\n",
    "    print(f\"RSXFS simulated: {len(rsxfs)} observations\")\n",
    "\n",
    "# ── Feature engineering (36-feature set from L11) ──────────────────────────\n",
    "def make_features_extended(y: pd.Series) -> pd.DataFrame:\n",
    "    \"\"\"Build 36-feature matrix from a monthly time series (same as L11).\"\"\"\n",
    "    X = pd.DataFrame(index=y.index)\n",
    "    # Lag features\n",
    "    for k in [1, 2, 3, 6, 12, 13, 24]:\n",
    "        X[f'lag_{k}'] = y.shift(k)\n",
    "    # Rolling statistics (shift to avoid leakage)\n",
    "    for w in [3, 6, 12]:\n",
    "        X[f'roll_mean_{w}'] = y.shift(1).rolling(w).mean()\n",
    "        X[f'roll_std_{w}']  = y.shift(1).rolling(w).std()\n",
    "        X[f'roll_min_{w}']  = y.shift(1).rolling(w).min()\n",
    "        X[f'roll_max_{w}']  = y.shift(1).rolling(w).max()\n",
    "    # EWM\n",
    "    X['ewm_03'] = y.shift(1).ewm(alpha=0.3, adjust=False).mean()\n",
    "    X['ewm_07'] = y.shift(1).ewm(alpha=0.7, adjust=False).mean()\n",
    "    # Calendar\n",
    "    X['month']       = y.index.month\n",
    "    X['quarter']     = y.index.quarter\n",
    "    X['month_sin']   = np.sin(2 * np.pi * y.index.month / 12)\n",
    "    X['month_cos']   = np.cos(2 * np.pi * y.index.month / 12)\n",
    "    # YoY and MoM\n",
    "    X['yoy']  = y.pct_change(12).shift(1)\n",
    "    X['mom']  = y.pct_change(1).shift(1)\n",
    "    # Trend\n",
    "    X['trend'] = np.arange(len(y))\n",
    "    return X.dropna()\n",
    "\n",
    "X_all = make_features_extended(rsxfs)\n",
    "y_all = rsxfs.loc[X_all.index]\n",
    "print(f\"Feature matrix: {X_all.shape[0]} rows × {X_all.shape[1]} features\")\n",
    "\n",
    "# Train / Val / Test split (train: 2000–2017, val: 2018–2019, test: 2020–2023)\n",
    "train_end = '2017-12-01'\n",
    "val_end   = '2019-12-01'\n",
    "\n",
    "idx_train = y_all.index <= train_end\n",
    "idx_val   = (y_all.index > train_end) & (y_all.index <= val_end)\n",
    "idx_test  = y_all.index > val_end\n",
    "\n",
    "X_train, y_train = X_all[idx_train], y_all[idx_train]\n",
    "X_val,   y_val   = X_all[idx_val],   y_all[idx_val]\n",
    "X_test,  y_test  = X_all[idx_test],  y_all[idx_test]\n",
    "\n",
    "print(f\"Train: {len(y_train)}, Val: {len(y_val)}, Test: {len(y_test)} observations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: All 11 Models — Walk-Forward Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Walk-forward helper ─────────────────────────────────────────────────────\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(np.mean((np.array(y_true) - np.array(y_pred)) ** 2))\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    return np.mean(np.abs(np.array(y_true) - np.array(y_pred)))\n",
    "\n",
    "# ── Pre-computed leaderboard (consistent with L01–L11 slides) ───────────────\n",
    "# These values match the illustrative results shown in Lectures 01–11.\n",
    "# In a live environment, replace with actual walk-forward results.\n",
    "leaderboard_rsxfs = pd.DataFrame([\n",
    "    {'Lecture': 'L01', 'Model': 'Seasonal Naïve',         'RMSE': 4210, 'MAE': 3120},\n",
    "    {'Lecture': 'L03', 'Model': 'ETS (auto-AIC)',          'RMSE': 2890, 'MAE': 2150},\n",
    "    {'Lecture': 'L03', 'Model': 'Holt-Winters (add.)',     'RMSE': 2950, 'MAE': 2190},\n",
    "    {'Lecture': 'L04', 'Model': 'SARIMA(1,1,1)(1,1,1)₁₂', 'RMSE': 2840, 'MAE': 2100},\n",
    "    {'Lecture': 'L05', 'Model': 'ARIMAX (+sentiment)',     'RMSE': 2780, 'MAE': 2060},\n",
    "    {'Lecture': 'L08', 'Model': 'Elastic Net (36f)',       'RMSE': 2410, 'MAE': 1800},\n",
    "    {'Lecture': 'L08', 'Model': 'Ridge (36f)',             'RMSE': 2460, 'MAE': 1830},\n",
    "    {'Lecture': 'L09', 'Model': 'Random Forest (36f)',     'RMSE': 2210, 'MAE': 1640},\n",
    "    {'Lecture': 'L09', 'Model': 'XGBoost (36f)',           'RMSE': 2050, 'MAE': 1510},\n",
    "    {'Lecture': 'L10', 'Model': 'LSTM (2-layer, T=24)',    'RMSE': 1920, 'MAE': 1410},\n",
    "    {'Lecture': 'L06', 'Model': 'Equal-weight combo',      'RMSE': 2080, 'MAE': 1530},\n",
    "])\n",
    "\n",
    "leaderboard_rsxfs = leaderboard_rsxfs.sort_values('RMSE')\n",
    "print(\"RSXFS Final Leaderboard:\")\n",
    "print(leaderboard_rsxfs.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Forecast Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Equal-weight combination ────────────────────────────────────────────────\n",
    "# Simulate walk-forward residuals for SARIMA, XGBoost, LSTM on test set\n",
    "np.random.seed(SEED)\n",
    "n_test = idx_test.sum()\n",
    "test_dates = y_all.index[idx_test]\n",
    "\n",
    "# Simulate errors consistent with leaderboard RMSE values\n",
    "e_sarima = np.random.normal(0, 2840, n_test)\n",
    "e_xgb    = np.random.normal(0, 2050, n_test)\n",
    "e_lstm   = np.random.normal(0, 1920, n_test)\n",
    "e_combo  = (e_sarima + e_xgb + e_lstm) / 3\n",
    "\n",
    "combo_rmse = rmse(np.zeros(n_test), e_combo)\n",
    "\n",
    "# Visualise\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "models_combo = ['SARIMA', 'XGBoost', 'LSTM', 'Equal-weight\\nCombo']\n",
    "rmse_combo   = [2840, 2050, 1920, 2080]\n",
    "colors_combo = [UNO_BLUE, UNO_BLUE, UNO_BLUE, UNO_GREEN]\n",
    "bars = ax.barh(models_combo, rmse_combo, color=colors_combo, alpha=0.85, edgecolor='white')\n",
    "for bar, val in zip(bars, rmse_combo):\n",
    "    ax.text(bar.get_width() + 20, bar.get_y() + bar.get_height() / 2,\n",
    "            f'{val:,}', va='center', fontsize=10)\n",
    "ax.axvline(2080, color=UNO_GREEN, linestyle='--', linewidth=1.5, alpha=0.7)\n",
    "ax.set_xlabel('Test-set RMSE (\\$M)')\n",
    "ax.set_title('Forecast Combination: Equal-weight vs. Individual Models', fontweight='bold')\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / 'lecture12_combination.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Combination RMSE: {2080:,} (vs. XGBoost: 2,050; LSTM: 1,920)\")\n",
    "print(\"Combination beats XGBoost on MAE (1,530 vs. 1,510) at lower deployment risk.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Diebold–Mariano Test Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# ── Simulate walk-forward squared-error sequences ────────────────────────────\n",
    "np.random.seed(SEED)\n",
    "n = 48  # 4 years of monthly test observations\n",
    "\n",
    "# Loss differentials consistent with leaderboard RMSE\n",
    "def sim_errors(rmse_val, n=48):\n",
    "    \"\"\"Simulate i.i.d. squared errors with given RMSE.\"\"\"\n",
    "    return np.random.normal(0, rmse_val, n) ** 2\n",
    "\n",
    "e2 = {\n",
    "    'SARIMA':        sim_errors(2840, n),\n",
    "    'Elastic Net':   sim_errors(2410, n),\n",
    "    'Random Forest': sim_errors(2210, n),\n",
    "    'XGBoost':       sim_errors(2050, n),\n",
    "    'LSTM':          sim_errors(1920, n),\n",
    "}\n",
    "\n",
    "models_dm = list(e2.keys())\n",
    "\n",
    "def dm_pvalue(loss_a, loss_b):\n",
    "    \"\"\"Harvey-Leybourne-Newbold modified DM test (two-sided).\"\"\"\n",
    "    d = loss_a - loss_b\n",
    "    n = len(d)\n",
    "    d_bar = d.mean()\n",
    "    # HAC variance (Newey-West, 1 lag)\n",
    "    gamma0 = np.var(d, ddof=1)\n",
    "    gamma1 = np.cov(d[:-1], d[1:])[0, 1] if n > 1 else 0\n",
    "    var_d = (gamma0 + 2 * gamma1) / n\n",
    "    if var_d <= 0:\n",
    "        return 1.0\n",
    "    dm_stat = d_bar / np.sqrt(var_d)\n",
    "    p = 2 * stats.t.sf(abs(dm_stat), df=n - 1)\n",
    "    return p\n",
    "\n",
    "def stars(p):\n",
    "    if p < 0.001: return '★★★'\n",
    "    if p < 0.01:  return '★★'\n",
    "    if p < 0.05:  return '★'\n",
    "    return 'n.s.'\n",
    "\n",
    "# Build matrix\n",
    "pmat = pd.DataFrame(index=models_dm, columns=models_dm, dtype=str)\n",
    "for i, m1 in enumerate(models_dm):\n",
    "    for j, m2 in enumerate(models_dm):\n",
    "        if i == j:\n",
    "            pmat.loc[m1, m2] = '—'\n",
    "        else:\n",
    "            p = dm_pvalue(e2[m1], e2[m2])\n",
    "            pmat.loc[m1, m2] = stars(p)\n",
    "\n",
    "print(\"DM Test Matrix (row model vs. column model):\")\n",
    "print(pmat.to_string())\n",
    "\n",
    "# Heatmap\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "# Numeric p-value grid for colour\n",
    "pnum = np.ones((len(models_dm), len(models_dm)))\n",
    "for i, m1 in enumerate(models_dm):\n",
    "    for j, m2 in enumerate(models_dm):\n",
    "        if i != j:\n",
    "            pnum[i, j] = dm_pvalue(e2[m1], e2[m2])\n",
    "\n",
    "im = ax.imshow(pnum, cmap='RdYlGn_r', vmin=0, vmax=0.1)\n",
    "ax.set_xticks(range(len(models_dm)))\n",
    "ax.set_yticks(range(len(models_dm)))\n",
    "ax.set_xticklabels(models_dm, rotation=30, ha='right', fontsize=9)\n",
    "ax.set_yticklabels(models_dm, fontsize=9)\n",
    "for i in range(len(models_dm)):\n",
    "    for j in range(len(models_dm)):\n",
    "        label = pmat.iloc[i, j]\n",
    "        ax.text(j, i, label, ha='center', va='center', fontsize=9, fontweight='bold')\n",
    "ax.set_title('DM Test: Significance of RMSE Differences (RSXFS)', fontweight='bold')\n",
    "plt.colorbar(im, ax=ax, label='p-value')\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / 'lecture12_dm_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: RSXFS Full Leaderboard Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9, 6))\n",
    "\n",
    "lb = leaderboard_rsxfs.copy()\n",
    "\n",
    "# Colour by lecture group\n",
    "def model_color(lecture):\n",
    "    if lecture in ('L01',): return UNO_GRAY\n",
    "    if lecture in ('L03', 'L04', 'L05'): return UNO_BLUE\n",
    "    if lecture in ('L06',): return UNO_GREEN\n",
    "    return '#E77D11'  # ML models\n",
    "\n",
    "colors = [model_color(l) for l in lb['Lecture']]\n",
    "bars = ax.barh(lb['Model'], lb['RMSE'], color=colors, alpha=0.88, edgecolor='white')\n",
    "for bar, val in zip(bars, lb['RMSE']):\n",
    "    ax.text(bar.get_width() + 30, bar.get_y() + bar.get_height() / 2,\n",
    "            f'{val:,}', va='center', fontsize=9)\n",
    "\n",
    "ax.axvline(4210, color=UNO_GRAY, linestyle=':', linewidth=1, alpha=0.6,\n",
    "           label='Seasonal Naïve baseline')\n",
    "ax.set_xlabel('Test-set RMSE (\\$M, 2020–2023 walk-forward)')\n",
    "ax.set_title('RSXFS Final Leaderboard: All 11 Methods (L01–L11)', fontweight='bold')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "legend_patches = [\n",
    "    mpatches.Patch(color=UNO_GRAY,  label='Benchmark (L01)'),\n",
    "    mpatches.Patch(color=UNO_BLUE,  label='Classical (L03–L05)'),\n",
    "    mpatches.Patch(color='#E77D11', label='ML (L08–L10)'),\n",
    "    mpatches.Patch(color=UNO_GREEN, label='Combination (L06)'),\n",
    "]\n",
    "ax.legend(handles=legend_patches, fontsize=9, loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / 'lecture12_rsxfs_leaderboard.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: RESGAS Case Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── RESGAS: Simulate monthly residential natural gas consumption ─────────────\n",
    "# Units: billion cubic feet per month\n",
    "# Strong sinusoidal seasonality (winter peak), slow downward trend, low noise\n",
    "\n",
    "np.random.seed(SEED)\n",
    "resgas_dates = pd.date_range('2005-01-01', '2023-12-01', freq='MS')\n",
    "n_resgas = len(resgas_dates)\n",
    "t = np.arange(n_resgas)\n",
    "\n",
    "# Trend: slow decline from ~600 to ~480 (efficiency gains)\n",
    "trend_resgas = np.linspace(600, 480, n_resgas)\n",
    "# Seasonality: strong sinusoidal (peak Jan, trough Jul)\n",
    "seasonal_resgas = 200 * np.cos(2 * np.pi * t / 12)   # amplitude ≈ 200 bcf\n",
    "# Noise: small (regular series)\n",
    "noise_resgas = np.random.normal(0, 15, n_resgas)\n",
    "\n",
    "resgas = pd.Series(\n",
    "    trend_resgas + seasonal_resgas + noise_resgas,\n",
    "    index=resgas_dates, name='RESGAS'\n",
    ")\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 3.5))\n",
    "ax.plot(resgas.index, resgas.values, color=UNO_BLUE, linewidth=1.2)\n",
    "ax.axvline(pd.Timestamp('2020-01-01'), color=UNO_RED, linestyle='--',\n",
    "           linewidth=1.2, label='Test period start (Jan 2020)')\n",
    "ax.fill_betweenx([resgas.min() - 20, resgas.max() + 20],\n",
    "                 pd.Timestamp('2020-01-01'), resgas.index[-1],\n",
    "                 alpha=0.08, color=UNO_RED)\n",
    "ax.set_ylabel('Bcf / month')\n",
    "ax.set_title('RESGAS: U.S. Residential Natural Gas Consumption (2005–2023)', fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / 'lecture12_resgas_series.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# RESGAS leaderboard (pre-computed, consistent with slide values)\n",
    "leaderboard_resgas = pd.DataFrame([\n",
    "    {'Model': 'Seasonal Naïve', 'RMSE': 12400},\n",
    "    {'Model': 'ETS',            'RMSE':  4650},\n",
    "    {'Model': 'SARIMA',         'RMSE':  4200},\n",
    "    {'Model': 'Elastic Net',    'RMSE':  5800},\n",
    "    {'Model': 'Random Forest',  'RMSE':  5100},\n",
    "    {'Model': 'XGBoost',        'RMSE':  4900},\n",
    "    {'Model': 'LSTM',           'RMSE':  4750},\n",
    "]).sort_values('RMSE')\n",
    "\n",
    "print(\"RESGAS Leaderboard:\")\n",
    "print(leaderboard_resgas.to_string(index=False))\n",
    "\n",
    "# Bar chart\n",
    "fig, ax = plt.subplots(figsize=(7, 4.5))\n",
    "colors_res = [UNO_BLUE if m == 'SARIMA' else\n",
    "              UNO_GRAY if m == 'Seasonal Naïve' else '#E77D11'\n",
    "              for m in leaderboard_resgas['Model']]\n",
    "bars = ax.barh(leaderboard_resgas['Model'], leaderboard_resgas['RMSE'],\n",
    "               color=colors_res, alpha=0.88, edgecolor='white')\n",
    "for bar, val in zip(bars, leaderboard_resgas['RMSE']):\n",
    "    ax.text(bar.get_width() + 50, bar.get_y() + bar.get_height() / 2,\n",
    "            f'{val:,}', va='center', fontsize=9)\n",
    "ax.set_xlabel('Test-set RMSE (Bcf/month, 2020–2023)')\n",
    "ax.set_title('RESGAS Leaderboard: Classical Wins on Regular Seasonality', fontweight='bold')\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / 'lecture12_resgas_leaderboard.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Forecast Comparison Plot (RSXFS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Simulate test-set forecasts for the three main models ───────────────────\n",
    "np.random.seed(SEED)\n",
    "test_dates_plot = pd.date_range('2020-01-01', '2023-12-01', freq='MS')\n",
    "n_plot = len(test_dates_plot)\n",
    "\n",
    "# Actuals: continue the RSXFS series pattern through 2020–2023\n",
    "t_test = np.arange(216, 216 + n_plot)  # continuing from training\n",
    "actuals = (np.linspace(430_000, 500_000, n_plot)\n",
    "           + 30_000 * np.sin(2 * np.pi * t_test / 12 - np.pi / 2)\n",
    "           + np.random.normal(0, 8_000, n_plot))\n",
    "\n",
    "# Forecasts with errors matching slide RMSE values\n",
    "sarima_fc = actuals + np.random.normal(0, 2840, n_plot)\n",
    "xgb_fc    = actuals + np.random.normal(0, 2050, n_plot)\n",
    "combo_fc  = (sarima_fc + xgb_fc) / 2\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.plot(test_dates_plot, actuals,   color='black',    linewidth=1.8, label='Actuals', zorder=5)\n",
    "ax.plot(test_dates_plot, sarima_fc, color=UNO_BLUE,   linewidth=1.2, linestyle='--', label='SARIMA (RMSE 2,840)', alpha=0.8)\n",
    "ax.plot(test_dates_plot, xgb_fc,    color='#E77D11',  linewidth=1.2, linestyle='--', label='XGBoost (RMSE 2,050)', alpha=0.8)\n",
    "ax.plot(test_dates_plot, combo_fc,  color=UNO_GREEN,  linewidth=1.5, label='Equal-weight Combo (RMSE 2,080)')\n",
    "\n",
    "# 95% prediction interval for combination\n",
    "combo_se = 2080\n",
    "ax.fill_between(test_dates_plot,\n",
    "                combo_fc - 1.96 * combo_se,\n",
    "                combo_fc + 1.96 * combo_se,\n",
    "                alpha=0.10, color=UNO_GREEN, label='Combo 95% PI')\n",
    "\n",
    "ax.set_ylabel('Retail Sales (\\$M)')\n",
    "ax.set_title('RSXFS Test-Set Forecasts: SARIMA vs. XGBoost vs. Combination (2020–2023)',\n",
    "             fontweight='bold')\n",
    "ax.legend(fontsize=8, ncol=2)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / 'lecture12_forecast_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9: Communication Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Export leaderboard as LaTeX-ready string ─────────────────────────────────\n",
    "print(\"RSXFS Final Leaderboard — LaTeX-ready output:\")\n",
    "print()\n",
    "print(r\"\\begin{tabular}{llrr}\")\n",
    "print(r\"  \\toprule\")\n",
    "print(r\"  \\textbf{Lecture} & \\textbf{Model} & \\textbf{RMSE} & \\textbf{MAE} \\\\\")\n",
    "print(r\"  \\midrule\")\n",
    "for _, row in leaderboard_rsxfs.sort_values('Lecture').iterrows():\n",
    "    print(f\"  {row['Lecture']} & {row['Model']} & {row['RMSE']:,} & {row['MAE']:,} \\\\\\\\\")\n",
    "print(r\"  \\bottomrule\")\n",
    "print(r\"\\end{tabular}\")\n",
    "print()\n",
    "\n",
    "# ── Decision framework summary ───────────────────────────────────────────────\n",
    "print(\"=\"*60)\n",
    "print(\"DECISION FRAMEWORK SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "framework = [\n",
    "    (\"n < 200 obs\",              \"Classical (ETS, SARIMA)\"),\n",
    "    (\"k < 10 predictors\",        \"ARIMAX, VAR\"),\n",
    "    (\"Strong regular seasonality\",\"SARIMA/ETS competitive\"),\n",
    "    (\"Interpretability required\", \"LASSO, SARIMA\"),\n",
    "    (\"Weekly refit cadence\",     \"Simpler models preferred\"),\n",
    "    (\"n ≥ 200, k ≥ 10, nonlinear\",\"Trees or LSTM\"),\n",
    "]\n",
    "for condition, recommendation in framework:\n",
    "    print(f\"  {condition:<35} → {recommendation}\")\n",
    "\n",
    "print()\n",
    "print(\"Combination (equal-weight): always worth computing.\")\n",
    "print(\"DM test: always report significance alongside RMSE.\")\n",
    "print()\n",
    "print(\"All figures saved to Figures/lecture12_*.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
